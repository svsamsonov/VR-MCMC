\documentclass[bj]{imsart}
\makeatletter
\def\set@curr@file#1{%
  \begingroup
    \escapechar\m@ne
    \xdef\@curr@file{\expandafter\string\csname #1\endcsname}%
  \endgroup
}
\def\quote@name#1{"\quote@@name#1\@gobble""}
\def\quote@@name#1"{#1\quote@@name}
\def\unquote@name#1{\quote@@name#1\@gobble"}
\makeatother
\usepackage{graphics}


\usepackage{lineno,hyperref}
\usepackage{amsfonts,amssymb}
\usepackage{mathtools}
%\usepackage{mathbbol}
\usepackage{bbm}
\usepackage{multirow,algorithm2e}%
\usepackage{xargs}
%\usepackage{showlabels}

\def\PE{\mathsf{E}}
\def\PVar{\mathsf{Var}}
\newcommandx{\CPE}[3][1=]{{\mathsf E}_{#1}\left[\left. #2 \, \right| #3 \right]}
\newcommand{\ps}[2]{\langle #1, #2 \rangle}
\def\nset{\mathbb{N}}
\def\rset{\mathbb{R}}
\usepackage[textsize=footnotesize]{todonotes} % pour voir les commentaires

\newcommand{\eric}[1]{\todo[color=purple!10, author=Eric]{#1}}
\newcommand{\erici}[1]{\todo[inline,color=purple!10, author=Eric]{#1}}
\newcommand{\denis}[1]{\todo[color=green!10, author=Denis]{#1}}
\newcommand{\denisi}[1]{\todo[inline,color=green!10, author=Denis]{#1}}
\newcommand{\sergey}[1]{\todo[color=red!10, author=Sergey]{#1}}
\newcommand{\sergeyi}[1]{\todo[inline,color=red!10, author=Sergey]{#1}}

\def\rmd{\mathrm{d}}
\def\rme{\mathrm{e}}
\def\rset{\mathbb{R}}
\def\NtrainPath{T}
\def\TrainSet{\mathcal{T}}
\modulolinenumbers[5]
\newcommand{\proofendsign}{$\Box$}
\newtheorem{thm}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{cor}{Corollary}
\newtheorem{example}{Example}
\newenvironment{proof}{{\noindent \bf Proof }}
 {{\hspace*{\fill}\proofendsign\par\bigskip}}
\def\TV{\operatorname{TV}}
\newcommand{\jac}[1]{\operatorname{J} _{#1}}

%added command for indicator - SS
\newcommand{\indi}[1]{\mathbbm{1}_{#1}}
\newcommand{\indiacc}[1]{\mathbbm{1}_{\{#1\}}}

\newcommand*{\argmin}{\operatornamewithlimits{arg\,min}}
\newcommand*{\const}{\mathrm{const}}
\usepackage[lofdepth,lotdepth]{subfig}
\usepackage{graphicx}
\newcommand*{\ol}{\overline}

\newcommand{\tcr}[1]{\textcolor{red}{#1}}
\newcommand{\tvnorm}[1]{\| #1 \|_{\operatorname{TV}}}
\def\kerULA{\operatorname{Q}}
\def\eqsp{\,}
\begin{document}

\begin{frontmatter}
\title{Variance reduction for MCMC methods via martingale representations}
\runtitle{Variance reduction for MCMC}
%\thankstext{T1}{This research was }


\begin{aug}
\author{\snm{D. Belomestny}\thanksref{a,b}\ead[label=e1]{denis.belomestny@uni-due.de}}
\and
\author{\snm{E. Moulines}\thanksref{c, b}\ead[label=e2]{eric.moulines@polytechnique.edu}}
\and
\author{\snm{S. Samsonov}\thanksref{b}\ead[label=e3]{svsamsonov@hse.ru}}
\and
\author{\snm{N. Shagadatov}\thanksref{b}\ead[label=e4]{nshagadatov@edu.hse.ru}}



\runauthor{D. Belomestny et al}

\affiliation{Duisburg-Essen University, National Research University Higher School of Economics,  Ecole Polytechnique  and Duisburg-Essen University}

\address[a]{
Duisburg-Essen University,
Faculty of Mathematics,
D-45127 Essen
Germany }

\address[c]{
Centre de Math\'ematiques Appliqu\'ees, UMR 7641, Ecole Polytechnique,
France
}

\address[b]{
National University Higher School of Economics, Moscow, Russia
}

\end{aug}

\begin{abstract}
In this paper we propose an efficient variance reduction approach for  MCMC algorithms relying on a novel discrete time martingale representation for Markov chains. Our approach is fully non-asymptotic and does not require any type of ergodicity or special product structure of the underlying density.  By rigorously analyzing the  convergence properties of the proposed algorithm, we show that it's complexity is indeed asymptotically smaller than one of the original MCMC algorithm. The numerical performance of the new method is illustrated  in the case of Gaussian mixtures and Bayesian regression models.
\end{abstract}

\begin{keyword}[class=MSC]
\kwd[Primary ]{60G40}
\kwd{60G40}
\kwd[; secondary ]{91G80}
\end{keyword}

\begin{keyword}
\kwd{MCMC}
\kwd{variance reduction}
\kwd{martingale representation}
\end{keyword}

\end{frontmatter}

\section{Introduction}

Monte Carlo integration typically has an error variance of the form
$\sigma^{2}/n,$ where $n$ is a sample size and \(\sigma^2\) is the variance of  integrand. We can make the variance smaller
by using a larger value of $n$.  Alternatively,
 we can  reduce $\sigma^2$ instead of increasing the sample size \(n\). To this end, one can try to construct
a new Monte Carlo experiment with the same expectation as the original one
but with a lower variance $\sigma^2$. Methods to achieve this are known as variance
reduction techniques. Variance reduction plays an important role in
Monte Carlo and Markov Chain Monte Carlo methods. Introduction to many of the variance reduction techniques can be found in \cite{christian1999monte}, \cite{rubinstein2016simulation}, \cite{GobetBook} and \cite{glasserman2013monte}. Recently one witnessed a revival of interest in efficient variance reduction methods  for MCMC algorithms, see for example \cite{dellaportas2012control}, \cite{mira2013zero}, \cite{brosse2018diffusion} and references therein.
\par
Suppose that we wish to compute $\pi(f):=\mathsf{E}\left[f(X)\right]$, where $X$
is a random vector-valued in $\mathcal{X}\subseteq\mathbb{R}^{d}$ with a density $\pi$ and $f:$
$\mathcal{X}\to\mathbb{R}$ with $f\in L^2(\pi)$.
The idea of the control variates variance reduction method
 is to find a cheaply computable random variable $\zeta$ with $\mathsf{E}[\zeta]=0$ and \(\mathsf{E}[\zeta^2]<\infty,\)
such that the variance of the r.v. $f(X)-\zeta$ is small.  The complexity of the problem of constructing classes $Z$ of control variates \(\zeta\) satisfying   $\mathsf{E}[\zeta]=0$ essentially depends on the degree of our knowledge on \(\pi.\)
For example, if \(\pi\) is analytically known and satisfies some regularity conditions, one can apply the well-known technique of  polynomial interpolation to construct control variates enjoying  some optimality properties, see, for example, Section~3.2 in \cite{dimov2008monte}. Alternatively, if an orthonormal system in \(L^2(\pi)\) is analytically available, one can build control variates \(\zeta\) as a linear combination of the corresponding basis functions, see \cite{GobetCV}. Furthermore, if \(\pi\) is known only up to a normalizing constant (which is often the case in Bayesian statistics), one can apply the recent approach of control variates  depending only on the gradient \(\nabla \log \pi\)  using Schr\(\ddot{\text{o}}\)dinger-type Hamiltonian operator in  \cite{mira2013zero} and Stein operator in \cite{brosse2018diffusion}.
In some situations \(\pi\) is not known analytically, but \(X\) can be represented as a function of  simple random variables with known distribution.
Such  situation arises, for example, in the case of functionals of  discretized diffusion processes. In this case a Wiener chaos-type decomposition can be used to construct control variates with nice theoretical properties, see \cite{belomestny2018stratified}.
Note that in order to compare different  variance reduction approaches, one has to analyze their complexity, that is, the number of numerical operations required to achieve a prescribed magnitude of the resulting variance.


The situation becomes much more difficult in the case of MCMC algorithms, where one  has to work with a
Markov chain \(X_p,\) \(p=0,1,2,\ldots,\) whose marginal distribution  converges  to \(\pi\) as time grows. One important class of the variance reduction methods in this case  is based on the Poisson equation for the corresponding  Markov chain. It was observed in~\cite{henderson1997variance}  that if a time-homogeneous Markov chain \((X_p)\) is stationary with stationary distribution \(\pi,\) then for any real-valued function \(G \in L^1(\pi) \) defined on the state space of the Markov chain \((X_p),\)  the function \(U(x) := G(x)-\mathsf{E}[G(X_{1})|X_0 = x]\) has zero mean with respect to \(\pi\).  The best choice for the function \(G\) corresponds to a solution of the Poisson equation  \(\mathsf{E}[G(X_{1})|X_0 = x]-G(x)=-f(x)+\pi(f)\).  Moreover, it is also related to the minimal asymptotic variance in the corresponding central limit theorem, see \cite{duncan2016variance} and \cite{mira2013zero}.   Although the Poisson equation involves the quantity of interest \(\pi(f)\)  and can not be  solved explicitly in most cases, this idea still can be used to construct some  approximations for the optimal zero-variance control variates. For example,  \cite{henderson1997variance} proposed to compute approximations for the solution of the Poisson equation for specific Markov chains with particular emphasis on models arising in stochastic network theory. In \cite{dellaportas2012control} and \cite{brosse2018diffusion}  series-type control variates are introduced and studied for reversible Markov chains. It is assumed in \cite{dellaportas2012control}  that the one-step conditional expectations  can be computed explicitly  for a set of basis functions. The authors in \cite{brosse2018diffusion} proposed another approach tailored to diffusion setting which does not require the computation of integrals of basis functions and only involves  applications of the underlying generator.
\par
In this paper we focus on the  Langevin type algorithms which got much attention recently, see \cite{dalalyan2017theoretical,durmus:moulines:2017, MR2353037, MR3861816, MR2977986} and references therein. We propose  a generic variance reduction method for these and other types algorithms, which is purely non-asymptotic and does not require that    the conditional expectations of the corresponding Markov chain can be computed  or that the generator is known analytically. Moreover, we do not need to assume stationarity or/and sampling under the invariant distribution \(\pi.\) We rigorously analyse the convergence of the method and study its complexity. It is shown that  our variance-reduced Langevin algorithm outperforms the standard Langevin algorithms in terms of complexity.
\par
The paper is organized as follows.  In Section~\ref{sec:setup} we set up the problem and introduce some notations. Section~\ref{seq:mart_repr} contains a novel martingale representation and shows how this representation can be used for variance reduction. In Section~\ref{sec:ula_analysis} we analyze the performance of the proposed variance reduction algorithm in the case of Unadjusted Langevin Algorithm (ULA). Section~\ref{sec:coeff} studies the complexity of the variance reduced ULA. Finally, numerical examples are presented in Section~\ref{sec:num}.

\section{Setup}\label{sec:setup}
Let  \(\mathcal{X}\) be a domain in \( \mathbb{R}^d.\)  Our aim is to numerically compute  expectations of the form
\[
\pi(f)=\int_{\mathcal{X}} f(x)\pi(\rmd x),
\]
where \(f:\) \(\mathcal{X}\longrightarrow \mathbb{R}\) and \(\pi\) is a probability measure supported on \(\mathcal{X}.\)
If  the dimension of the space \(\mathcal{X}\) is large and \(\pi(f)\) can not be computed analytically, one can apply Monte Carlo methods. However, in many practical situations  direct sampling from \(\pi\) is impossible and this precludes the use of plain Monte Carlo methods in this case. One popular alternative to Monte Carlo  is Markov Chain Monte Carlo (MCMC), where one is looking for a discrete time  (possibly non-homogeneous) Markov chain   \((X_p)_{p\geq 0}\) such that \(\pi\) is its unique invariant measure. In this paper we study a class of MCMC algorithms with \((X_p)_{p\geq 0}\) satisfying the  the following recurrence relation:
\begin{equation}
\label{eq:chain_gen}
X_{p}=\Phi_{p}(X_{p-1},\xi_{p}),\quad p=1,2,\ldots ,\quad X_{0}=x_0,
\end{equation}
for some i.i.d.  random vectors \(\xi_p\in \mathbb{R}^m\) with distribution \(P_{\xi}\)
and some Borel-measurable
functions $\Phi_{p}\colon\mathcal{X}\times\mathbb{R}^{m}\to\mathcal{X}.$
In fact, this is quite general class of Markov chains (see Theorem~1.3.6 in ~\cite{moulines2018})
and many well-known MCMC algorithms can be represented in form \eqref{eq:chain_gen}.
Let us consider two popular examples.
\begin{example}[Unadjusted Langevin Algorithm]
\label{exam:langevin-algorithm}
Fix a sequence of positive time steps \((\gamma_p)_{p\geq 1}.\) Given a Borel function $\mu\colon\mathbb{R}^{d}\to\mathbb{R}^{d}$,
consider a non-homogeneous
discrete-time Markov chain $(X_{p})_{p\geq0}$ defined by
\begin{equation}\label{eq:chain}
X_{p+1}=X_{p}-\gamma_{p+1}\mu(X_{p})+\sqrt{2 \gamma_{p+1}}Z_{p+1},\end{equation}
where $\left(Z_{p}\right)_{p\geq1}$ is an i.i.d.\ sequence of $d$-dimensional
standard Gaussian random vectors. If
$\mu= \nabla U$
for some continuously differentiable function $U,$ then Markov chain~\eqref{eq:chain} can be used to approximately sample from the density
\begin{equation}\label{eq:stationary_distr}
\pi(x)= \mathrm{const}\,\rme^{-U(x)}, \quad \mathrm{const}= \left. 1 \middle /{\int_{\mathbb{R}^{d}} \rme^{-U(x)}\, \rmd x,} \right.
\end{equation}
provided that \(\int_{\mathbb{R}^{d}} \rme^{-U(x)}\,\rmd x\) is finite. This method is usually referred to as
Unadjusted Langevin Algorithm (ULA).
In fact the Markov chain~\eqref{eq:chain}
arises as the Euler-Maruyama discretization
of the Langevin diffusion
\[
\rmd Y_t=-\mu(Y_t)\,\rmd t+ \rmd W_t
\]
with nonnegative time steps $(\gamma_p)_{p\ge1}$,
and,  under mild technical conditions, the latter Langevin diffusion admits $\pi$
of~\eqref{eq:stationary_distr}
as its unique invariant distribution; see \cite{dalalyan2017theoretical} and \cite{durmus:moulines:2017}.
\end{example}
\begin{example}[Metropolis-Adjusted Langevin Algorithm]
The Metropolis-Hastings algorithm
associated with a target density \(\pi\) requires the choice of a sequence of conditional densities  \((q_p)_{p\geq 1}\) also called proposal or candidate kernels. The transition from the value  of the Markov chain \(X_p\)  at time \(p\)
and its value at time \(p + 1\) proceeds via the following transition step:

\begin{algorithm}[H]
Given \(X_p=x\)\;
\begin{enumerate}
\item Generate \(Y_p\sim q_p(\cdot|x)\)\;
\item Put
\begin{eqnarray*}
X_{p+1}=
\begin{cases}
Y_p, & \texttt{ with probability } \alpha(x,Y_p),
\\
x, &  \texttt{ with probability } 1-\alpha(x,Y_p),
\end{cases}
\end{eqnarray*}
where
\begin{eqnarray*}
\alpha(x,y)=\min\left\{1,\frac{\pi(y)}{\pi(x)}\frac{q_p(x|y)}{q_p(y|x)}\right\}.
\end{eqnarray*}
\end{enumerate}
\end{algorithm}
This transition is reversible with respect to \(\pi\) and therefore preserves the stationary density \(\pi\); see \cite[Chapter~2]{moulines2018}. If \(q_p\) have a wide enough
support to eventually reach any region
of the state space \(\mathcal{X}\) with positive mass
under \(\pi\), then this transition is irreducible and $\pi$ is a maximal irreducibility measure \cite{mengersen:tweedie:1996}. The  Metropolis-Adjusted Langevin algorithm (MALA) takes  \eqref{eq:chain} as proposal, that is,
\begin{eqnarray*}
q_p(y|x)=(\gamma_{p+1})^{-d/2}\boldsymbol{\varphi}\Bigl([y-x+\gamma_{p+1}\mu(x)]/\sqrt{\gamma_{p+1}}\Bigr),
\end{eqnarray*}
where
$\boldsymbol{\varphi}(z)=(2\pi)^{-d/2} \exp\{-|z|^2/2\}$,
$z\in\mathbb R^d$, denotes the density of a $d$-dimensional
standard Gaussian random vector.  The MALA algorithms usually provide noticeable speed-ups in convergence for most problems. It is not difficult to see that the MALA chain can be compactly represented in the form
\begin{align*}
X_{p+1} &=X_p+\mathbbm{1}\bigl(U_{p+1}\leq \alpha(X_{p},Y_{p})\bigr)(Y_{p}-X_p),  \,
Y_{p}&=X_p-\gamma_{p+1}\mu(X_p)+\sqrt{\gamma_{p+1}}Z_{p+1},
\end{align*}
where \((U_{p})_{p\geq 1}\) is an i.i.d. sequence of uniformly distributed on \([0,1]\) random variables independent of \((Z_p)_{p\geq 1}.\) Thus we recover \eqref{eq:chain_gen} with  \(\xi_p=(U_p,Z_p)\in \mathbb{R}^{d+1}\) and
\begin{eqnarray*}
\Phi_p(x,(u,z)^\top)=x+\mathbbm{1}\bigl(u\leq \alpha(x,x-\gamma_{p}\mu(x)+\sqrt{\gamma_{p}}z)\bigr)(-\gamma_{p}\mu(x)+\sqrt{\gamma_{p}}z).
\end{eqnarray*}
\end{example}
\begin{example}
  Let \((X_t)_{t\in [0,T]}\) be the unique strong solution to a  SDE of the form:
\begin{eqnarray}
\label{eq:sde-inv}
\rmd X_t=b(X_t)\, \rmd t+\sigma(X_t)\, \rmd W_t,\quad t\geq 0,	
\end{eqnarray}
where \(W\) is a standard \(\mathbb{R}^m\)-valued Brownian motion, \(b:\) \(\mathbb{R}^d\to \mathbb{R}^d\) and \(\sigma:\) \(\mathbb{R}^d\times \mathbb{R}^m \to \mathbb{R}^d\) are locally Lipschitz continuous functions with at most linear growth.
The process \((X_t)_{t\geq 0}\) is a Markov
process and let \(L\) denote its infinitesimal generator defined by
\begin{equation*}
Lg=b^\top \nabla g+\frac{1}{2}\operatorname{Tr}(\sigma^\top D^2g\sigma)
\end{equation*}
\eric{do we need a trace here ? I think also that $D^2$ is not defined }
for any \(g\in C^2(\mathbb{R}^d).\)
If there exists a continuously twice differentiable Lyapunov function \(V:\) \(\mathbb{R}^d\to \mathbb{R}_{+}\) such that
\begin{eqnarray*}
\sup_{x\in \mathbb{R}^d} LV(x) <\infty,\quad \limsup_{|x|\to \infty} LV(x)<0,
\end{eqnarray*}
then there is an invariant probability measure \(\pi\)  for \(X,\) that is, \(X_t\sim \pi\) for all \(t> 0\) if \(X_0\sim \pi.\) Invariant measures are crucial in the study of the long term behaviour of stochastic differential
systems \eqref{eq:sde-inv}.    Under some additional assumptions,  the invariant
measure \(\pi\) is  ergodic and this property
can be exploited  to compute  the integrals \(\pi(f)\) for \(f\in L^2(\pi)\) by means of ergodic averages. The idea is to replace the diffusion \(X\) by a (simulable) discretization scheme of the form (see e.g. \cite{MR3861816})
\begin{eqnarray*}
\bar X_{n+1}=\bar X_n+\gamma_{n+1} b(\bar X_n)+\sigma(\bar X_n)(W_{\Gamma_{n+1}}-W_{\Gamma_n}), \quad n\geq 0,\quad \bar X_0=X_0,
\end{eqnarray*}
where \(\Gamma_n=\gamma_1+\ldots+\gamma_n\) and \((\gamma_n)_{n\geq 1}\) is a non-increasing sequence of time steps. Then for a function \(f\in L^2(\pi)\) we can approximate \(\pi(f)\) via
\begin{eqnarray*}
\pi_n^\gamma(f)=\frac{1}{\Gamma_n}\sum_{i=1}^n \gamma_{i}f(\bar X_{i-1}).
\end{eqnarray*}
Due to typically high correlation between \(X_0,X_1,\ldots\)  variance reduction is of crucial importance here.
As a matter of fact, in many cases there is no explicit formula for the invariant measure and this makes the use of gradient  based  variance reduction techniques (see e.g. \cite{mira2013zero})   impossible in this case.
On the contrary, our method can be directly used to reduce the variance of the ergodic estimator \(\pi_n^\gamma\) without explicit knowledge of \(\pi.\)
\end{example}
\section{Martingale representation and variance reduction}
\label{seq:mart_repr}
In this section we give a general discrete-time martingale representation for  Markov chains of the type  \eqref{eq:chain_gen} which is  used below to construct an efficient variance reduction algorithm. Let \((\phi_k)_{k\in \mathbb{Z}_+}\) be a complete orthonormal system in \(L^2(\mathbb{R}^m, P_{\xi})\) with \(\phi_0\equiv 1\).  In particular, we have
\begin{equation*}
\mathsf{E}[\phi_i(\xi)\phi_j(\xi)]=\delta_{ij},\quad i,j\in  \mathbb{Z}_{+}
\end{equation*}
with \(\xi \sim P_{\xi}.\)
Notice that this implies that the random variables
$\phi_k(\xi)$, $k\ge1$, are centered. As an example, we can take  multivariate Hermite polynomials for the ULA algorithm and a tensor product of Shifted Legendre polynomials for "uniform part" and Hermite polynomials for "Gaussian part"  of the random variable $\xi = (u, z)^T$ in MALA, as the Shifted Legendre polynomials are orthogonal with respect to the Lebesgue measure on \([0,1].\) In the sequel, we denote by $(\mathcal{G}_p)_{p\in \mathbb{Z}_+}$  the filtration
generated by generated by $(\xi_p)_{p \in \mathbb{N}^*}$ with the convention $\mathcal{G}_0=\mathrm{triv}$.
\erici{It would be perhaps interesting to state the conditions satisfied by the functions $(\Phi_l)_{l \geq 1}$ here}
\begin{thm}\label{prop:29032018a1}
 Let $f$ be a Borel function $\mathbb{R}^{d}\to\mathbb R$ such that
$\mathsf{E}\left[\left|f(X_{p})\right|^{2}\right]<\infty$,   where $(X_{p})_{p\geq 0}$ is a Markov chain of the form \eqref{eq:chain_gen} for some sequence of functions $(\Phi_l)_{l\geq 1}.$
Then,
for $p>j$, the following representation holds in \(L^2(P)\)
\eric{I would write \tcr{$L^2(\mathsf{P})$} to be coherent with the notation $\mathsf{E}$ and we need to define what is the underlying probability space}
\begin{eqnarray}
\label{eq:mart_repr}
f(X_{p})=\mathsf{E}\left[\left.f(X_{p})\right|\mathcal G_{j}\right]+\sum_{k=1}^{\infty}\sum_{l=j+1}^{p}a_{p,l,k}(X_{l-1})\phi_k\left(\xi_{l}\right),
\end{eqnarray}
where
\begin{eqnarray}
\label{eq:coeff_mart}
a_{p,l,k}(x)=\mathsf{E}\left[\left.f(X_{p})\phi_k\left(\xi_{l}\right)\right|X_{l-1}=x\right], \quad p\geq l, \quad k\in \mathbb{N}.
\end{eqnarray}
\end{thm}
\begin{cor}
If  all the kernels $\Phi_l,$ $l\geq 1,$ are equal,  then  the representation \eqref{eq:mart_repr} takes the form
\begin{eqnarray*}
f(X_{p})=\mathsf{E}\left[\left.f(X_{p})\right|\mathcal G_{j}\right]+\sum_{k=1}^{\infty}\sum_{l=j+1}^{p}\bar a_{p-l,k}(X_{l-1})\phi_k\left(\xi_{l}\right)
\end{eqnarray*}
with
\begin{eqnarray*}
\bar a_{r,k}(x)=\mathsf{E}\left[\left.f(X_{r})\phi_k\left(\xi_{1}\right)\right|X_{0}=x\right], \quad r, k\in \mathbb{N}.
\end{eqnarray*}

\end{cor}
\begin{proof}[Proof of Theorem~\ref{prop:29032018a1}]
The expansion obviously holds for $p=1$ and $j=0$.
Indeed, since $\left(\phi_{k}\right)_{k \geq 0}$ is a complete orthonormal system of , we have for all $x \in \rset^d$,
\eric{it is clear, but we must say in which sense this equality should be understood}
\[
f(X^x_{1})=\PE[f(X_{1}^x)]+\sum_{k\geq1}a_{1,1,k}(x)\phi_{k}(\xi_{1})
\]
with $a_{1,1,k}(x)=\PE[f(X_{1}^x)\phi_{k}(\xi_{1})]$,
provided $\PE[|f(X^x_{1})|^{2}]<\infty.$
\tcr{Recall that $\mathcal{G}_l=\sigma(\xi_{1},\ldots,\xi_{l}),$
$l=1,2,\ldots, $
and $\mathcal{G}_0=\mathrm{triv}$.}
\eric{do we need this, this is recalled ten lines above}
Suppose that (\ref{eq:mart_repr})
holds for $p=q$, all $j<q$, and all Borel-measurable functions
$f$ with $\mathsf{E}\left[|f(X_{q})|^2\right]<\infty$.
Let us prove it for $p=q+1$.
Given $f$ with $\mathsf{E}\left[|f(X_{p})|^2\right]<\infty$,
due to the orthonormality and completeness
of the system $\left(\phi_{k}\right)$, we get by
conditioning on $\mathcal{G}_{q}$,
\[
f(X_{p})=\mathsf{E}\left[\left.f(X_{p})\right|{\mathcal{G}}_q\right]+\sum_{k\geq1}\alpha_{p,q+1,k}\phi_{k}(\xi_{q+1}),
\]
where
\begin{equation*}
\alpha_{p,q+1,k}
=\mathsf{E}\left[\left.f(X_{p})\phi_{k}(\xi_{q+1})\right|\mathcal G_q\right].
\end{equation*}
By the Markov property of $(X_{l})$,
we have
$\mathsf{E}[f(X_{p})|\mathcal{G}_q]
=\mathsf{E}[f(X_{p})|X_{q}]$.
Furthermore, a calculation involving
intermediate conditioning on $\mathcal G_{q+1}$
and the recurrence relation
$X_{q+1}=\Phi_{q+1}(X_{q},\xi_{q+1})$
verifies that
\[
\alpha_{p,q+1,k}=\mathsf{E}\left[\left.f(X_{p})\phi_{k}(\xi_{q+1})\right|X_{q}\right]
=a_{p,q+1,k}(X_{q})
\]
for suitably chosen
Borel-measurable functions $a_{p,q+1,k}$.
We thus arrive at
\begin{align}
\label{berm:sig_X}
f(X_{p})=\mathsf{E}\left[\left.f(X_{p})\right|X_{q}\right]+\sum_{k\geq1}a_{p,q+1,k}(X_{q})\phi_{k}(\xi_{q+1}),
\end{align}
which is the required statement in the case $j=q$.
Now assume $j<q$.
The random variable
$\mathsf{E}\left[\left.f(X_{p})\right|X_{q}\right]$
is square integrable and has the form
$g(X_{q})$,
hence the induction hypothesis applies, and we get
\begin{equation}\label{eq:28082017a1}
\mathsf{E}\left[\left.f(X_{p})\right|X_{q}\right]=\mathsf{E}\left[\left.f(X_{p})\right|X_{j}\right]+\sum_{k\geq1}\sum_{l=j+1}^{q}a_{p,l,k}(X_{l-1})\phi_{k}(\xi_{l})
\end{equation}
with
\begin{align*}
a_{p,l,k}(X_{l-1}) &= \mathsf{E}\left[\left.\mathsf{E}\left[\left.f(X_{p})\right|\mathcal{G}_{q}\right]\phi_{k}(\xi_{l})\right|\mathcal{G}_{l-1}\right]
= \mathsf{E}\left[\left.f(X_{p})\phi_{k}(\xi_{l})\right|\mathcal{G}_{l-1}\right]\\
&=\mathsf{E}\left[\left.f(X_{p})\phi_{k}(\xi_{l})\right|X_{l-1}\right].
\end{align*}
Formulas \eqref{berm:sig_X}
and~\eqref{eq:28082017a1} conclude the proof.
\end{proof}
From  numerical point of view another representation of the coefficients \(a_{p,l,k}\)  turns out to be more useful.
\begin{prop}
The coefficients \(a_{p,l,k}\) in \eqref{eq:coeff_mart}  can be alternatively represented as
\begin{eqnarray*}
a_{p,l,k}(x)=\mathsf{E}\left[\phi_k\left(\xi\right)Q_{p,l}\left(\Phi_l(x,\xi)\right)\right]
\end{eqnarray*}
with \(Q_{p,l}(x)=\mathsf{E}\left[\left.f(X_{p})\right|X_{l}=x\right],\) \(p\geq l.\)
%where   a r.v. \(\xi\) independent of \(\mathcal{G}_{l-1}.\)
The functions \((Q_{p,l})_{l=0}^p\)  can be computed by the backward recurrence: $Q_{p,p}(x)=f(x)$ and for $l \in \{0,\ldots,p-1\}$
\begin{eqnarray}
\label{eq:qpl}
Q_{p,l}(x)=\mathsf{E}\left[\left.Q_{p,l+1}(X_{l+1})\right|X_{l}=x\right].
%=\mathsf{E}[Q_{p,l+1}(\Phi_{l+1}(x,\xi))]
\end{eqnarray}
In the case $\Phi_l=\Phi$ for all $l\geq 1,$ we have
\begin{eqnarray*}
\bar a_{r,k}(x)=\mathsf{E}\left[\phi_k\left(\xi\right)Q_{r}\left(\Phi(x,\xi)\right)\right]
\end{eqnarray*}
with  \(Q_{r}(x)=\mathsf{E}\left[\left.f(X_{r})\right|X_{0}=x\right],\) $r\in \mathbb{N}.$
\end{prop}
\par
Next we show how  the representation \eqref{eq:mart_repr} can be used to  reduce the variance of MCMC algorithms.  Consider the case of time homogeneous transition kernels and introduce a weighted average estimator $\pi_{n}^{N}(f)$ of the form
\begin{equation}\label{eq:29032018a2}
\pi_{n}^{N}(f)=\frac{1}{n}\sum_{p=N+1}^{N+n} f(X_{p}),
\end{equation}
where $N\in\mathbb N_0$ is the length of the burn-in period and $n\in\mathbb N$
the number of effective samples.
Given $N$ and $n$ as above, for $K\in\mathbb N$, denote
\begin{eqnarray}
\nonumber
M_{K,n}^N(f) &=&\frac{1}{n}\sum_{p=N+1}^{N+n}\left[\sum_{k=1}^{K}\sum_{l=1}^{p} \bar a_{p-l,k}(X_{l-1})\phi_k(\xi_{l})\right]
\\
&=& \sum_{k=1}^{K}\sum_{l=N+1}^{N+n}\left(1+\frac{N-l}{n}\right)A_{N+n-l,k}(X_{l-1})\phi_{k}\left(\xi_{l}\right),
\label{eq:29032018a5}
\end{eqnarray}
where
\[
A_{s,k}(x)=\frac{1}{s}\sum_{r=0}^{s}\bar a_{r,k}(x).
\]
Since \(X_{l-1}\) is independent of \(\xi_{l}\) and \tcr{\(\mathsf{E}[\phi_k(\xi_{l})]=0,\) \(k\neq 1,\)} \eric{I do not understand this}  the r.v.  \(M_{K,n}^N(f)\) has zero mean and can be viewed as a control variate.
The representation \eqref{eq:coeff_mart} suggests that the variance of  the  variance-reduced estimator
\begin{equation}
\label{eq:29032018a3}
\pi_{K,n}^N(f)=\pi_n^N(f)-M_{K,n}^N(f)
\end{equation}
should be small for \(K\) large enough. Indeed, since $\PE[ \phi_k(\xi_l) \phi_k'(\xi_l)]=0$ if $k \ne k'$, we obtain
\[
\mathsf{Var}[\pi_{K,n}^N(f)]\leq\sum_{k=K+1}^{\infty}\sum_{l=1}^{n}\mathsf{E}[A^2_{n-l,k}(X_{N+l-1})]
\]
and $\mathsf{Var}[\pi_{K,n}^N(f)]$ is small provided that the coefficients $A_{s,k}$ decay fast enough as $k\to \infty.$
In the next section   we provide a detailed   analysis of  $\mathsf{Var}[\pi_{K,n}^N(f)]$  for the  ULA  algorithm (see Example~\ref{exam:langevin-algorithm}).
\section{Analysis of variance reduced ULA}
\label{sec:ula_analysis}
In this section we perform the convergence analysis of the ULA algorithm. For the sake of clarity and notational simplicity we restrict our attention to the constant time step, that is, we take $\gamma=\gamma_1=\gamma_2=\ldots.$
\tcr{We shall use the notations $\nset=\{1,2,\ldots\}$ and $\nset_0=\mathbb N\cup\{0\}$.} \eric{these notations are used before; it is best to put all the notations in a short paragraph just after the introduction}
By $H_k$, $k\in\mathbb N_0$,
we denote the normalized Hermite polynomial on $\mathbb R$, that is,
$$
H_k(x)=\frac{(-1)^k}{\sqrt{k!}}\rme^{x^2/2}\frac{\partial^k}{\partial x^k}\rme^{-x^2/2},
\quad x\in\mathbb R.
$$
For a multi-index $\mathbf{k}=(k_i)\in\mathbb N_0^d$,
$\mathbf{H}_\mathbf{k}$ denotes the normalized Hermite polynomial on $\mathbb R^d$, that is,
$$
\mathbf{H}_\mathbf{k}(\mathbf{x})=\prod_{i=1}^d H_{k_i}(x_i),\quad \mathbf{x}=(x_i)\in\mathbb R^d.
$$
In what follows, we also use the notation
$|\mathbf{k}|=\sum_{i=1}^d k_i$ for $\mathbf{k}=(k_i)\in\mathbb N_0^d$,
and we set $\mathcal G_p=\sigma(Z_1,\ldots,Z_p)$, $p\in\mathbb N$, and $\mathcal G_0=\mathrm{triv}$.
Given $N$ and $n$ as above, for $K\in\mathbb N$, denote
\begin{align}
M_{K,n}^N(f) & =\sum_{\mathbf k\colon 0<\|\mathbf{k}\|\le K}\sum_{l=1}^{n}\Bigl(1-\frac{l}{n}\Bigr)A_{n-l,\mathbf{k}}(X_{N+l-1})\mathbf{H}_\mathbf{k}(Z_{N+l})
\label{eq:29032018a5}
\notag
\end{align}
with \(\|\mathbf{k}\|=\max_{i} k_i.\)
For an estimator $\rho(f)\in\{\pi_n^N(f),\pi_{K,n}^N(f)\}$
of $\pi(f)$ (see \eqref{eq:29032018a2} and \eqref{eq:29032018a3}), we shall be interested in the Mean Squared Error (MSE),
which can be decomposed as the sum of the squared  bias
and the  variance:
\begin{equation}
\label{eq:29032018a4}
\mathrm{MSE}\left[\rho(f)\right] = \PE\left[\left\{\rho(f)-\pi(f)\right\}^2\right]
=\left\{\PE[\rho(f)]-\pi(f)\right\}^2 +\PVar[\rho(f)].
\end{equation}
Our analysis is carried out under the following two assumptions:
\begin{description}
\item[(H1)][\textbf{Lipschitz continuity}] The potential $U$ is differentiable and $\nabla U$ is Lipschitz, that is, there exists $L_U < \infty$ such that
\begin{eqnarray*}
|\nabla U(x)-\nabla U(y)|\leq L_U|x-y|, \quad x,y\in \rset^d.
\end{eqnarray*}
\item[(H2)][\textbf{Convexity outside a ball}] There exist $K_U>0,$ $M_U>0$ and $m_U>0$ such that for any $x\not\in B_{K_U}(0)$  it holds
\begin{equation*}
\ps{D^2U(x)}{x} \geq (m_U/2)\|x\|^2.
\end{equation*}
\end{description}
First we analyse the biases of the estimators
$\pi_n^N(f)$ and $\pi_{K,n}^N(f).$
\paragraph{Squared bias:} Due to the martingale transform structure
of $M_{K,n}^N(f)$,
we have
\[
\mathsf E\left[M_{K,n}^N(f)\right]=0,
\]
Hence both estimators
$\pi_n^N(f)$ and $\pi_{K,n}^N(f)$ have the same conditional bias.
Notice that this remains true also when we substitute
the coefficients $a_{p,l,\mathbf{k}}$ in~\eqref{eq:29032018a5}
with some independent approximations $\widehat a_{p,l,\mathbf{k}}.$

Denote by $\pi($ denotes the probability measure on $\mathbb R^d$
with density $\pi$ of~\eqref{eq:stationary_distr};
for $\gamma>0$, define the Markov kernel $Q_\gamma$ associated to one-step of the ULA algorithm by
\[
\kerULA_{\gamma}(x,A)=\int_{A} \frac{1}{(4 \pi \gamma)^{d/2}} \text{ exp} \left\{ -\frac{1}{4\gamma} \left\| y - x + \gamma \mu(x)\right\|^2\right\} \rmd y.
\]
\eric{what is the notation convention here; I do not like much $\mathcal{Q}$ but $Q$ is already define and it is perhaps not a great idea to supersede the previous notation}
For a bounded Borel function~$f$,
we can estimate the conditional bias
similarly to~\cite[Section~4]{durmus:moulines:2017}:
\begin{equation}
\label{eq:06042018a2}
\left\{\mathsf{E}[\pi_{K,n}^N(f)]-\pi(f)\right\}^2
=\left(\mathsf{E}[\pi_{n}^N(f)]-\pi(f)\right)^2
\le
\frac{\mathrm{osc}(f)^2}{n}\sum_{p=N+1}^{N+n}
\tvnorm{\delta_x \kerULA_\gamma^{p}-\pi(\cdot)}^2,
\end{equation}
where
$\mathrm{osc}(f):=\sup_{x\in\mathbb R^d}f(x)-\inf_{x\in\mathbb R^d}f(x)$,
$\tvnorm{\mu-\nu}$ denotes the total variation distance
between probability measures $\mu$ and $\nu$, that is,
\[
\tvnorm{\mu-\nu}=\sup_{A\in\mathcal B(\mathbb R^d)}
|\mu(A)-\nu(A)| \eqsp.
\]
Different specific upper bounds for the squared bias
can be deduced from~\eqref{eq:06042018a2}
using results of Section~3 in~\cite{durmus:moulines:2017}
on bounds in the total variation distance. It is known  that under assumptions {\bf (H1)} and {\bf (H2)},   the corresponding Markov chain has a unique stationary distribution $\pi_\gamma$, which is different from $\pi$. Moreover, as shown in Proposition, \tcr{(Appendix)}
\eric{put a precise reference here}
 \begin{equation}
\label{eq:bias}
\tvnorm{\delta_x\kerULA_{\gamma}^{n}-\pi_\gamma} \leq C_1\rho^{\gamma n}\left(V(x) + \pi_\gamma(V)\right), \quad \tvnorm{\pi-\pi_\gamma} \leq C_2 \sqrt{\gamma}
\end{equation}
for  $V(x) = 1 + \|x\|^2,$ some $\rho\in (0,1)$ and  constants $C_1,$ $C_2>0$ independent of $\gamma$  and $n.$
\paragraph{Variance:} An upper bound for the variance of the classical estimator~\eqref{eq:29032018a2}
under {\bf (H1)} and {\bf (H2)} is derived in Section.  In particular, for any bounded function $f,$
\begin{equation}
\label{eq:var-mc}
\PVar[\pi_n^N(f)]\lesssim (n\gamma)^{-1} \eqsp,
\end{equation}
where $\lesssim$ stands for inequality up to a constant depending on constants $L_U,$ $m_U,$  $M_U$ and $\|f\|_\infty.$
One of the main results of this paper is the following  upper bound for the variance of $\pi_{K,n}^{N}(f).$
We start by introducing some notations.
For $m\in\mathbb N$, a smooth function
$h\colon\mathbb R^{d\times m}\to\mathbb R$
with arguments being denoted
$(y_1,\ldots,y_m)$, $y_i\in\mathbb R^d$, $i=1,\ldots,m$,
a multi-index $\mathbf k=(k_i)\in\mathbb N_0^d$,
and $j\in\{1,\ldots,m\}$,
we use the notation $\partial^{\mathbf k}_{y_j} h$ for the multiple derivative of $h$
with respect to the components of~$y_j$:
\[
\partial^{\mathbf k}_{j} h(y_1,\ldots,y_m)
:=\partial^{k_d}_{y_j^d}
\ldots
\partial^{k_1}_{y_j^1}
h(y_1,\ldots,z_m),
\quad y_j=(y_j^1,\ldots,y_j^d).
\]
In the particular case $m=1$ we can drop
the subscript $y_1$ in that notation.
\begin{thm}\label{th:mr}
Assume {\bf (H1)} and {\bf (H2)}.
Suppose additionally that a bounded function $f$ and $\mu=\nabla U$ are $K\geq 2$ times continuously differentiable
and for all $x\in\mathbb R^d$, $j \in \{1,\dots,d\}$ and  \(\mathbf{k}\) satisfying \(0<\|\mathbf{k}\|\leq  K,\)
\begin{equation}
\label{eq:smooth-mu}
|\partial_j^{\mathbf{k}} f(x)|\le  B_f, \quad |\partial_j^{\mathbf{k}} \mu (x)|\leq B_\mu \eqsp.
\end{equation}
Then it holds
\begin{eqnarray}
\label{eq:var-bound}
\PVar\left(\pi_{K,n}^{N}(f)\right)\lesssim n^{-1} \gamma^{K-2} \eqsp,
\end{eqnarray}
where \tcr{\(\lesssim\) stands for inequality up to a constant not depending  on $\gamma,$ \(n\) and \(N.\)}
\eric{this is defined less precisely just above}
\end{thm}
Let us sketch the main steps of the proof. First using integration by parts we prove that
\begin{eqnarray}
\label{eq:Ask-part}
A_{s,\mathbf{k}}(x)=\gamma^{|\mathbf{k}'|/2}\frac{\sqrt{(\mathbf{k}-\mathbf{k}')!}}{\sqrt{\mathbf{k}!}}\mathsf{E}\left[ \partial_{Z_1}^{\mathbf{k}'} F(x,Z_1,\ldots,Z_s) \mathbf{H}_{\mathbf{k}-\mathbf{k}'}(Z_{1})\right]
\end{eqnarray}
where $F_s(X_0,Z_1,\ldots,Z_s)=s^{-1}\sum_{r=0}^s f(X_r)$ and $\partial_{Z_1}^{\mathbf{k}'}$
stands for a weak partial derivative of the functional $F_s$ that also can be viewed as discretised version of Malliavin derivative. From \eqref{eq:Ask-part}, we derive
\begin{eqnarray*}
\sum_{\mathbf{k}\colon\|\mathbf{k}\|\geq K+1} A^2_{s,\mathbf{k}}(x)\leq \sum_{I\subseteq\{1,\ldots,d\},\, I\neq \emptyset}
\left(\frac{\gamma}{2}\right)^{|I|K}
\PVar_x\left( s^{-1} \sum\nolimits_{p=1}^{s}\partial_{y_{1}}^{\mathbf{K}_I}f\left(X_{p}\right)
\right),
\end{eqnarray*}
where the sum runs over all nonempty subsets $I$ of the set $\{1,\ldots,d\}$ and
\[
\mathbf{\mathbf{K}}_{I}=K(\indi{I}(1),\ldots, \indi{I}(d)).
\]
 Finally we show via  the Poincare inequality that under smoothness assumption \eqref{eq:smooth-mu} it holds
 \eric{Since we do note state was $W_s$ is, it is a bit difficult to guess why we are using a Poincare inequality there}
\begin{equation*}
\PVar_x\left(s^{-1} \sum\nolimits_{p=1}^{s}\partial_{y_{1}}^{\mathbf{K}_I}f\left(X_{p}\right)
\right)\leq W_s(x)
\end{equation*}
for all $x$ and some family of functions $(W_s)_{s\geq 1}$ such that  the sum $\sum_{s=1}^n \mathsf{E}[W_s(X_{N+n-s-1})]$ is bounded uniformly in $n\in \mathbb{N}.$
\section{Proofs}
\label{sec:proofs}
\subsection{Proof of Theorem~\ref{th:mr}}
For $l\le p$, we have the representation
\[
X_{p}=G_{p}(X_0,\sqrt{\gamma}Z_{1},\ldots,\sqrt{\gamma}Z_{p}),
\]
where the function \(G_{p}:\) \(\mathbb{R}^{d\times(p+1)}\to \mathbb{R}^{d}\) is defined as
\begin{equation}
\label{eq:definition-G-p-l}
G_{p}(x,y_1,\ldots,y_p):=\Phi(\cdot,y_{p})\circ\Phi(\cdot,y_{p-1})\circ\ldots\circ\Phi(x,y_{1})
\end{equation}
with, for $x,y\in\mathbb R^d$,   $\Phi(x,y)=x-\gamma\mu(x)+y$. 
As a consequence, for a function $f\colon\mathbb R^d\to\mathbb R$
as in Section~\ref{sec:setup}, we have
$$
f\left(X_{p}\right) =f\circ G_{p}(X_{0},\sqrt{\gamma}Z_{1},\ldots,\sqrt{\gamma}Z_{p}).
$$
In what follows, for $\mathbf{k}\in\mathbb N_0^d$,
we use the shorthand notation
\begin{equation}
\label{eq:definition-differential-f-p}
\partial_{1}^{\mathbf k} f\left(X_{p}\right)
:=\partial_{1}^{\mathbf k} [f\circ G_{p}](X_{0},\sqrt{\gamma}Z_{1},\ldots,\sqrt{\gamma}Z_{p})
\end{equation}
whenever the function $f\circ G_{p}$ is smooth enough
(that is, $f$ and $\mu$ need to be smooth enough).
Finally, for a multi-index $\mathbf{k}=(k_i)\in\mathbb N_0^d$, we use the notation
\(\mathbf{k} ! :=k_1!\cdot\ldots\cdot k_d !\)

\begin{lem}\label{eq:a_repr}
Fix some \(\mathbf{k},\mathbf{k}'\in \mathbb{N}_0^d\) with \(\mathbf{k}'\le \mathbf{k}\) componentswise.  Then the following representation holds
$$
\bar a_{p,\mathbf{k}}(x)=\left(\gamma^{|\mathbf{k}'|}\frac{(\mathbf{k}-\mathbf{k}') !}{\mathbf{k}!}\right)^{1/2}
\mathsf E\left[\left.
\partial_{1}^{\mathbf{k}'}f(X_p)\mathbf{H}_{\mathbf{k}-\mathbf{k}'}(Z_l)\right|X_{0}=x\right].
$$
\end{lem}

\begin{proof}
Let $\boldsymbol{\varphi}(z)=\frac{1}{(2\pi)^{d/2}} \exp\{-|z|^2/2\}$,
$z\in\mathbb R^d$, denote the density of a $d$-dimensional
standard Gaussian random vector.
We first remark that, for the normalized Hermite polynomial $\mathbf H_{\mathbf k}$ on $\mathbb R^d$,
$\mathbf k\in\mathbb N_0^d$, it holds
$$
\mathbf{H}_{\mathbf{k}}(z)\boldsymbol{\varphi}(z)
=\frac{(-1)^{|\mathbf{k}|}}{\sqrt{\mathbf{k} !}} \partial^{\mathbf{k}} \boldsymbol{\varphi}(z).
$$
This enables to use the integration by parts in vector form as follows
(below $\prod_{j=l+1}^p:=1$ whenever $l=p$)
\begin{align*}
&\bar a_{p,\mathbf{k}}(x)
 =
\int_{\mathbb R^d}\ldots\int_{\mathbb R^d}
f\circ G_{p}(x,\sqrt{\gamma}z_{1},\ldots,\sqrt{\gamma}z_{p})
\mathbf{H}_{\mathbf{k}}(z_{1})\boldsymbol{\varphi}(z_1)
\prod_{j=2}^p\boldsymbol{\varphi}(z_j)\, dz_{1}\ldots dz_{p}
\\
& =
\frac{1}{\sqrt{\mathbf k!}}
\int_{\mathbb R^d}\ldots\int_{\mathbb R^d}
f\circ G_{p}(x,\sqrt{\gamma}z_{1},\ldots,\sqrt{\gamma}z_{p})
(-1)^{|\mathbf{k}|}\partial^{\mathbf{k}} \boldsymbol{\varphi}(z_1)
\prod_{j=2}^p\boldsymbol{\varphi}(z_j)\, dz_{1}\ldots dz_{p}
\\
& =
\frac{\gamma^{|\mathbf k'|/2}}{\sqrt{\mathbf k!}}
\int_{\mathbb R^d}\ldots\int_{\mathbb R^d}
\partial_{1}^{\mathbf k'}[f\circ G_{p}](x,\sqrt{\gamma}z_{1},\ldots,\sqrt{\gamma}z_{p})
(-1)^{|\mathbf{k}-\mathbf k'|}\partial^{\mathbf{k}-\mathbf k'} \boldsymbol{\varphi}(z_1)
\prod_{j=2}^p\boldsymbol{\varphi}(z_j)\, dz_{1}\ldots dz_{p}
\\
 & =\gamma^{|\mathbf{k}'|/2}\frac{\sqrt{(\mathbf{k}-\mathbf{k}')!}}{\sqrt{\mathbf{k}!}}\mathsf{E}\left[\partial_{y_{1}}^{\mathbf{k}'}[f\circ G_{p}](x,\sqrt{\gamma}Z_{1},\ldots,\sqrt{\gamma}Z_{p})\mathbf{H}_{\mathbf{k}-\mathbf{k}'}(Z_{1})\right].
\end{align*}
The last expression yields the result.
\end{proof}

For multi-indices $\mathbf k,\mathbf k'\in\mathbb N_0^d$
with $\mathbf k'\le\mathbf k$ componentwise
and $\mathbf k'\ne\mathbf k$,
we get applying first Lemma~\ref{eq:a_repr}
\begin{equation*}
A_{s,\mathbf{k}}(x)
=\left(\gamma^{|\mathbf{k}'|}\frac{(\mathbf{k}-\mathbf{k}')!}{\mathbf{k}!}\right)^{1/2}
\,
\mathsf{E}_x\left[
\frac{1}{s}\sum_{r=1}^{s}(\partial_{1}^{\mathbf{k}'} f(X_r)-\mathsf{E}_x\partial_{1}^{\mathbf{k}'}f(X_r))\mathbf{H}_{\mathbf{k}-\mathbf{k}'}(Z_1)\right]
\end{equation*}

Assume that $\mu$ and $f$ are $K\times d$ times continuously differentiable.
Then, given $\mathbf k\in\mathbb N_0^d$,
by taking $\mathbf k'= \mathbf k'(\mathbf k)
=(K1_{\{k_1>K\}},\ldots,K1_{\{k_d>K\}})$, we get
\begin{align}
\label{eq:sum_abar}
\sum_{\mathbf{k}\colon\|\mathbf{k}\|\geq K+1}A^2_{s,\mathbf{k}}(x) & =\sum_{\mathbf{k}\colon\|\mathbf{k}\|\geq K+1}\left(\gamma^{|\mathbf{k}'|}\frac{(\mathbf{k}-\mathbf{k}')!}{\mathbf{k}!}\right)Q_s(\mathbf{k}',\mathbf{k}-\mathbf{k}')\\
\nonumber
 & =\sum_{I\subseteq\{1,\ldots,d\},\, I\neq \emptyset}\gamma^{|I|K}\sum_{\mathbf{m}_{I}\in\mathbb{N}_{I}^{d}}\frac{\mathbf{m}_{I}!}{\left(\mathbf{m}_{I}+\mathbf{K}_{I}\right)!}
 \\
 \nonumber
 & \hspace{1em}\times\sum_{\mathbf{m}_{I^c}\in \mathbb{N}^d_{0,I^c},\,\|\mathbf{m}_{I^c}\|\leq K}Q_s(\mathbf{\mathbf{K}}_{I},\mathbf{m}_{I}+\mathbf{m}_{I^c}),
\end{align}
where for any two multi-indices \(\mathbf{r},\) \(\mathbf{q}\) from \(\mathbb{N}_0^d\)
\begin{eqnarray*}
Q_s(\mathbf{r},\mathbf{q})
=
\left(\mathsf{E}_x\left[\frac{1}{s}\sum_{p=1}^{s}\left(\partial_{y_{1}}^{\mathbf r}f\left(X_{p}\right)-\mathsf{E}_x\left[\partial_{y_{1}}^{\mathbf r}f\left(X_{p}\right)\right]\right)\mathbf{H}_{\mathbf{q}}(Z_{1})\right]\right)^{2}.
\end{eqnarray*}
In \eqref{eq:sum_abar} the first sum runs over all nonempty subsets $I$ of the set $\{1,\ldots,d\}.$
For any subset $I,$ $\mathbb{N}_{I}^{d}$ stands for a set
of multi-indices $\mathbf{m}_{I}$ with elements $m_{i}=0,$ $i\not\in I,$
and $m_{i}\in\mathbb{N},$  $i\in I.$ Moreover, \(I^c=\{1,\ldots,d\}\setminus I\) and \(\mathbb{N}^d_{0,I^c}\) stands for a set
of multi-indices $\mathbf{m}_{I^c}$ with elements $m_{i}=0,$ $i\in I,$
and $m_{i}\in\mathbb{N}_0,$  $i\not\in I.$ Finally, the multi-index \(\mathbf{K}_I\) is defined as $\mathbf{\mathbf{K}}_{I}=(K1_{\{1\in I\}},\ldots,K1_{\{d\in I\}}).$
Applying the estimate
\begin{eqnarray*}
\frac{\mathbf{m}_{I}!}{\left(\mathbf{m}_{I}+\mathbf{K}_{I}\right)!}\leq (1/2)^{|I| K},
\end{eqnarray*}
we get
\begin{eqnarray*}
\sum_{\mathbf{k}\colon\|\mathbf{k}\|\geq K+1}A^2_{s,\mathbf{k}}(x)&\leq &
\sum_{I\subseteq\{1,\ldots,d\},\, I\neq \emptyset} (\gamma_{l}/2)^{|I|K}
\\
&& \times\sum_{\mathbf{m}_{I}\in\mathbb{N}_{I}^{d}} \sum_{\mathbf{m}_{I^c}\in \mathbb{N}^d_{0,I^c},\,\|\mathbf{m}_{I^c}\|\leq K} Q(\mathbf{\mathbf{K}}_{I},\mathbf{m}_{I}+\mathbf{m}_{I^c})
\\
&\leq &
\sum_{I\subseteq\{1,\ldots,d\},\, I\neq \emptyset} (\gamma_{l}/2)^{|I|K} \sum_{\mathbf{m}\in\mathbb{N}_0^{d}} Q(\mathbf{\mathbf{K}}_{I},\mathbf{m}).
\end{eqnarray*}
Now using the consequence
$\sum_{\mathbf{m}\in \mathbb{N}^d_0} \langle\xi,\mathbf{H}_\mathbf{m}(Z_1)\rangle^2\le\langle\xi,\xi\rangle$
of the Parseval's identity,
we derive
\begin{eqnarray*}
\sum_{\mathbf{k}\colon\|\mathbf{k}\|\geq K+1}A^2_{s,\mathbf{k}}(x)
&\leq & \sum_{I\subseteq\{1,\ldots,d\},\, I\neq \emptyset}
\left(\frac{\gamma_{l}}{2}\right)^{|I|K}
\mathsf{Var}_x\left(\frac{1}{s}\sum_{p=1}^{s}\partial_{1}^{\mathbf{K}_I}f\left(X_{p}\right)
\right)
\end{eqnarray*}
Next we show that under the conditions  of Theorem~\ref{th:mr}
\begin{eqnarray*}
\mathsf{Var}_x\left(\frac{1}{s}\sum_{p=1}^{s}\partial_{1}^{\mathbf{K}_I}f\left(X_{p}\right)
\right)\leq W_s(x)
\end{eqnarray*}
for all $x$ and some family of functions $(W_s)_{s\geq 1}$ such that  the sum $\sum_{s=1}^n \mathsf{E}[W_s(X_{N+n-s-1})]$ is bounded uniformly in $n\in \mathbb{N}.$

 For the sake of simplicity we present the proof only in one-dimensional case. First we need to prove several auxiliary results.
\begin{lem}\label{lem:06062018a1}
Let $(x_p)_{p\in\mathbb N_0}$
and $(\epsilon_p)_{p\in\mathbb N}$
be sequences of nonnegative real numbers
satisfying $x_0=\ol C_0$ and
\begin{align}
0&\le x_p\le\alpha_p x_{p-1}+\gamma \epsilon_p,\quad p\in\mathbb N,
\label{eq:06062018a1}\\
0&\le\epsilon_p\le\ol C_1\prod_{k=1}^p \alpha_k^2,\quad p\in\mathbb N,
\label{eq:06062018a2}
\end{align}
where $\alpha_p,\gamma\in(0,1)$, $p\in\mathbb N$,
and $\ol C_0,\ol C_1$ are some nonnegative constants. Assume
\begin{equation}\label{eq:06062018a3}
\gamma \sum_{r=1}^\infty \prod_{k=1}^r \alpha_k\le\ol C_2
\end{equation}
for some constant $\ol C_2$. Then
$$
x_p\le(\ol C_0+\ol C_1\ol C_2)\prod_{k=1}^p \alpha_k,\quad p\in\mathbb N.
$$
\end{lem}

\begin{proof}
Applying~\eqref{eq:06062018a1} recursively, we get
$$
x_p\le\ol C_0\prod_{k=1}^p \alpha_k
+\gamma\sum_{r=1}^p \epsilon_r
\prod_{k=r+1}^p \alpha_k,
$$
where we use the convention $\prod_{k=p+1}^p:=1$.
Substituting estimate~\eqref{eq:06062018a2}
into the right-hand side, we obtain
$$
x_p\le\left(\ol C_0+\ol C_1
\gamma\sum_{r=1}^p  \prod_{k=1}^r \alpha_k
\right)
\prod_{k=1}^p \alpha_k,
$$
which, together with~\eqref{eq:06062018a3}, completes the proof.
\end{proof}

In what follows, we use the notation
\begin{equation}
\label{eq:definition-alpha}
\alpha_k=1-\gamma \mu'(X_{k-1}),\quad k\in\mathbb N.
\end{equation}

The assumption \eqref{eq:smooth-mu} implies that  $|\mu'(x)|\leq B_\mu$ for some constant $B_\mu>0$ and all $x\in \mathbb{R}^d$. Without loss of generality we suppose that $\gamma B_\mu<1.$
\begin{lem}\label{lem:06062018a2}
Under assumptions of Theorem~\ref{th:mr},
for all natural $r\le K$ and $l\le p$, there exist constants $C_r$ (not depending on $l$ and $p$) such that
\begin{equation}
\label{eq:08062018b2}
\left|\partial_{y_l}^r X_p \right| \le C_r\prod_{k=l+1}^p (1-\gamma\mu'(X_{k-1}))\quad \text{a.s.}
\end{equation}
where $\partial_{y_l}^r X_p$ is defined in \eqref{eq:definition-differential-f-p}. Moreover, we can choose $C_1=1$.
\end{lem}
\begin{lem}\label{lem:06062018a3}
Under assumptions of Theorem~\ref{th:mr},
for all natural $r\le K$, $j\ge l$ and $p>j$, we have
\begin{equation}\label{eq:08062018b3}
\left|\partial_{y_j} \partial_{y_l}^r X_p\right|
\le c_r\prod_{k=l+1}^p (1-\gamma\mu'(X_{k-1})), \quad \text{a.s.}
\end{equation}
with some constants $c_r$
not depending on $j$, $l$ and $p$,
while, for $p\le j$, it holds
$\partial_{y_{j}}\partial_{y_{l}}^{r}X_{p}=0$.
\end{lem}

\begin{proof}
The last statement is straightforward.
We fix natural numbers $j\ge l$ and prove~\eqref{eq:08062018b3}
for all $p>j$ by induction in~$r$.
First, for $p>j$, we write
$$
\partial_{y_{l}}X_{p}
=\left[1-\gamma\mu'(X_{p-1})\right]\partial_{y_{l}}X_{p-1}
$$
and differentiate this identity with respect to~$y_j$
$$
\partial_{y_{j}}\partial_{y_{l}}X_{p}
=\left[1-\gamma\mu'(X_{p-1})\right]\partial_{y_{j}}\partial_{y_{l}}X_{p-1}-\gamma\mu''(X_{p-1})\partial_{y_{j}}X_{p-1}\partial_{y_{l}}X_{p-1}.
$$
By Lemma~\ref{lem:06062018a2}, we have
\begin{align*}
|\partial_{y_{j}}\partial_{y_{l}}X_{p}|
&\le\alpha_p|\partial_{y_{j}}\partial_{y_{l}}X_{p-1}|
+\gamma
B_\mu
\prod_{k=l+1}^{p-1}\alpha_k
\prod_{k=j+1}^{p-1}\alpha_k\\
&\le\alpha_p|\partial_{y_{j}}\partial_{y_{l}}X_{p-1}|
+\gamma
\const
\prod_{k=l+1}^{j}\alpha_k
\prod_{k=j+1}^{p}\alpha_k^2,
\quad p\ge j+1,
\end{align*}
with a suitable constant.
By Lemma~\ref{lem:06062018a1} applied
to bound $|\partial_{y_{j}}\partial_{y_{l}}X_{p}|$
for $p\ge j+1$
(notice that $\partial_{y_j}\partial_{y_l}X_j=0$, that is,
$\ol C_0$ in Lemma~\ref{lem:06062018a1} is zero,
while $\ol C_1$ in Lemma~\ref{lem:06062018a1}
has the form $\const\prod_{k=l+1}^j \alpha_k$),
we obtain~\eqref{eq:08062018b3} for $r=1$.
The induction hypothesis is now that the inequality
\begin{equation}\label{eq:induc-derive}
\left|\partial_{y_{j}}\partial_{y_{l}}^{k}X_{p}\right|
\leq c_{k}\prod_{s=l+1}^{p}\alpha_{s}
\end{equation}
holds for all natural $k<r\;(\le K)$ and $p>j$.
We need to show~\eqref{eq:induc-derive} for $k=r$.
Fa\`a di Bruno's formula implies for $2\le r\le K$ and $p>l$
\begin{align}
\partial_{y_{l}}^{r}X_{p}
&=\left[1-\gamma\mu'(X_{p-1})\right]\partial_{y_{l}}^{r}X_{p-1}
\label{eq:08062018a1}\\
&\hspace{1em}-\gamma\sum\frac{r!}{m_{1}!\ldots m_{r-1}!\,}\mu^{(m_{1}+\ldots+m_{r-1})}(X_{p-1})\prod_{k=1}^{r-1}\left(\frac{\partial_{y_{l}}^{k}X_{p-1}}{k!}\right)^{m_{k}},
\notag
\end{align}
where the sum is taken over all $(r-1)$-tuples of nonnegative integers
$(m_{1},\ldots,m_{r-1})$ satisfying the constraint
\begin{equation}\label{eq:09062018a1}
1\cdot m_{1}+2\cdot m_{2}+\ldots+(r-1)\cdot m_{r-1}=r.
\end{equation}
Notice that we work with $(r-1)$-tuples
rather than with $r$-tuples
because the term containing
$\partial_{y_l}^r X_{p-1}$
on the right-hand side of~\eqref{eq:08062018a1}
is listed separately.
For $p>j$, we then have
\begin{align}
&\partial_{y_{j}}\partial_{y_{l}}^{r}X_{p}
=\left[1-\gamma_{p}\mu'(X_{p-1})\right]\partial_{y_{j}}\partial_{y_{l}}^{r}X_{p-1}-\gamma\mu''(X_{p-1})\partial_{y_{l}}^{r}X_{p-1}\partial_{y_{j}}X_{p-1}
\label{eq:09062018a2}\\
&\hspace{1em}-\gamma\sum\frac{r!}{m_{1}!\ldots m_{r-1}!\,}\mu^{(m_{1}+\ldots+m_{r-1}+1)}(X_{p-1})\partial_{y_{j}}X_{p-1}\prod_{k=1}^{r-1}\left(\frac{\partial_{y_{l}}^{k}X_{p-1}}{k!}\right)^{m_{k}}
\notag\\
&\hspace{1em}-\gamma\sum\frac{r!}{m_{1}!\ldots m_{r-1}!\,}\mu^{(m_{1}+\ldots+m_{r-1})}(X_{p-1})\partial_{y_{j}}\left[\prod_{k=1}^{r-1}\left(\frac{\partial_{y_{l}}^{k}X_{p-1}}{k!}\right)^{m_{k}}\right]
\notag\\
&\hspace{1em} =\left[1-\gamma\mu'(X_{p-1})\right]\partial_{y_{j}}\partial_{y_{l}}^{r}X_{p-1}+\gamma\epsilon_{l,j,p},
\notag
\end{align}
where the last equality defines the quantity $\epsilon_{l,j,p}$.
Furthermore,
\begin{eqnarray*}
\partial_{y_{j}}\left[\prod_{k=1}^{r-1}\left(\frac{\partial_{y_{l}}^{k}X_{p-1}}{k!}\right)^{m_{k}}\right]&=&\sum_{s=1}^{r-1}\frac{m_{s}}{s!}\left(\frac{\partial_{y_{l}}^{s}X_{p-1}}{s!}\right)^{m_{s}-1}\partial_{y_{j}}\partial_{y_{l}}^{s}X_{p-1}
\\
&& \times \prod_{k\le r-1,\,k\neq s}\left(\frac{\partial_{y_{l}}^{k}X_{p-1}}{k!}\right)^{m_{k}}.
\end{eqnarray*}
Using Lemma~\ref{lem:06062018a2},
induction hypothesis~\eqref{eq:induc-derive}
and the fact that $m_{1}+\ldots+m_{r-1}\ge2$
for $(r-1)$-tuples of nonnegative integers
satisfying~\eqref{eq:09062018a1},
we can bound $|\epsilon_{l,j,p}|$ as follows
\begin{align*}
&\left|\epsilon_{l,j,p}\right|  \leq  B_{\mu}C_{r}\prod_{k=l+1}^{p-1}\alpha_{k}\prod_{k=j+1}^{p-1}\alpha_{k}+B_{\mu}\sum\frac{r!}{m_{1}!\ldots m_{r-1}!\,}\left[\prod_{k=j+1}^{p-1}\alpha_{k}\right]
\\
&\times \prod_{s=1}^{r-1}\left(\frac{C_{s}\prod_{k=l+1}^{p-1}\alpha_{k}}{s!}\right)^{m_{s}}\\
&+B_{\mu}\sum\frac{r!}{m_{1}!\ldots m_{r-1}!\,}\sum_{t=1}^{r-1}\frac{m_t}{t!}\left(\frac{C_{t}\prod_{k=l+1}^{p-1}\alpha_{k}}{t!}\right)^{m_{t}-1}c_t\left[\prod_{k=l+1}^{p-1}\alpha_{k}\right]
\\
&\times \prod_{s\le r-1,\,s\neq t}\left(\frac{C_{s}\prod_{k=l+1}^{p-1}\alpha_{k}}{s!}\right)^{m_{s}}\leq \const\prod_{k=l+1}^{j}\alpha_{k}\prod_{k=j+1}^{p}\alpha_{k}^{2}
\end{align*}
with some constant ``$\const$'' depending on
$B_\mu,r,C_1,\ldots,C_r,c_1,\ldots,c_{r-1}$.
Thus, \eqref{eq:09062018a2} now implies
$$
|\partial_{y_{j}}\partial_{y_{l}}^{r}X_{p}|
\le\alpha_p|\partial_{y_{j}}\partial_{y_{l}}^{r}X_{p-1}|
+\gamma \,\const\prod_{k=l+1}^{j}\alpha_{k}\prod_{k=j+1}^{p}\alpha_{k}^{2},
\quad p\ge j+1.
$$
We can again apply Lemma~\ref{lem:06062018a1}
to bound $|\partial_{y_{j}}\partial_{y_{l}}^r X_{p}|$
for $p\ge j+1$
(notice that $\partial_{y_j}\partial_{y_l}^r X_j=0$, that is,
$\ol C_0$ in Lemma~\ref{lem:06062018a1} is zero,
while $\ol C_1$ in Lemma~\ref{lem:06062018a1}
has the form $\const\prod_{k=l+1}^j \alpha_k$),
and we obtain~\eqref{eq:induc-derive} for $k=r$.
This concludes the proof.
\end{proof}

\begin{lem}
\label{lem:var_poincare}
Under assumptions of Theorem~\ref{th:mr}, it holds
$$
\mathsf{Var}_x\left[\sum_{p=1}^{s}\partial_{y_{1}}^{K}f\left(X_{p}\right)\right]\le B_K\quad\text{a.s.},
$$
where $B_K$ is a deterministic bound that does not depend on $l$ and~$q$.
\end{lem}

\begin{proof}
The expression
$\sum_{p=1}^q \partial_{y_1}^K f(X_p)$
can be viewed as a deterministic function of
$X_{0},Z_1,Z_{2},\ldots,Z_q$
$$
\sum_{p=1}^q  \partial_{y_1}^K f(X_p)
=F(X_{0},Z_1,Z_{2},\ldots,Z_q).
$$
By the (conditional) Gaussian Poincar\'e inequality,
we have
$$
\mathsf{Var}_x\left[\sum_{p=1}^{q}\partial_{y_{1}}^{K}f\left(X_{p}\right)\right]
\le\mathsf E_x\left[
\|\nabla_Z F(X_{0},Z_1,Z_{2},\ldots,Z_q)\|^2
\right],
$$
where $\nabla_Z F=(\partial_{Z_1} F,\ldots,\partial_{Z_q} F)$,
and $\|\cdot\|$ denotes the Euclidean norm.
Notice that \(\partial_{Z_j} F=\sqrt{\gamma}\,\partial_{y_j} F\) and
hence
$$
\mathsf{Var}_x\left[\sum_{p=1}^{q}\partial_{y_{1}}^{K}f\left(X_{p}\right)\right]\leq\gamma\sum_{j=1}^{q}\mathsf{E}_x\left[\left(\sum_{p=1}^{q}\gamma\partial_{y_{j}}\partial_{y_{1}}^{K}f\left(X_{p}\right)\right)^{2}\right].
$$
It is straightforward to check that
$\partial_{y_j}\partial_{y_1}^K f(X_p)=0$
whenever $p<j$. Therefore, we get
\begin{equation}\label{eq:10062018a2}
\mathsf{Var}_x\left[\sum_{p=1}^{q}\partial_{y_{1}}^{K}f\left(X_{p}\right)\right]\leq\gamma\sum_{j=1}^{q}\mathsf{E}_x\left[\left(\sum_{p=j}^{q}\gamma \partial_{y_{j}}\partial_{y_{1}}^{K}f\left(X_{p}\right)\right)^{2}\right].
\end{equation}
Now fix $p$ and $j$, $p\ge j$, in $\{1,\ldots,q\}$.
By Fa\`a di Bruno's formula
\[
\partial_{y_{1}}^{K}f\left(X_{p}\right)=\sum\frac{K!}{m_{1}!\ldots m_{K}!}f^{(m_{1}+\ldots+m_{K})}(X_{p})\prod_{k=1}^{K}\left(\frac{\partial_{y_{1}}^{k}X_{p}}{k!}\right)^{m_{k}},
\]
where the sum is taken over all $K$-tuples of nonnegative integers
$(m_1,\ldots,m_K)$ satisfying
$$
1\cdot m_{1}+2\cdot m_{2}+\ldots+K\cdot m_{K}=K.
$$
Then
\begin{eqnarray*}
\partial_{y_{j}}\partial_{y_{1}}^{K}f\left(X_{p}\right)
&=&\sum\frac{K!}{m_{1}!\ldots m_{K}!}f^{(m_{1}+\ldots+m_{K}+1)}(X_{p})\left[\partial_{y_{j}}X_{p}\right]\prod_{k=1}^{K}\left(\frac{\partial_{y_{1}}^{k}X_{p}}{k!}\right)^{m_{k}} \\
&&+\sum\frac{K!}{m_{1}!\ldots m_{K}!}f^{(m_{1}+\ldots+m_{K})}(X_{p})
\sum_{s=1}^{K}\frac{m_{s}}{s!}
\left(\frac{\partial_{y_{1}}^{s}X_{p}}{s!}\right)^{m_{s}-1}
\\
&& \times\left[\partial_{y_{j}}\partial_{y_{1}}^{s}X_{p}\right]
\prod_{k\le K,\,k\neq s}\left(\frac{\partial_{y_{1}}^{k}X_{p}}{k!}\right)^{m_{k}}.
\end{eqnarray*}
Using the bounds of
Lemmas \ref{lem:06062018a2} and~\ref{lem:06062018a3},
we obtain
\begin{eqnarray}
\label{eq:part-part}
\left|\partial_{y_{j}}\partial_{y_{1}}^{K}f\left(X_{p}\right)\right|
\leq A_{K}\prod_{k=2}^{p}\alpha_{k}
\end{eqnarray}
with a suitable constant $A_{K}$.
Substituting this in~\eqref{eq:10062018a2},
we proceed as follows
\begin{align*}
\mathsf{Var}_x\left[\sum_{p=1}^{q}\partial_{y_{1}}^{K}f\left(X_{p}\right)\right]
&\le
\gamma^3 A_{K}^{2}\sum_{j=1}^{q}
\mathsf{E}\left(\sum_{p=j}^{q}\prod_{k=2}^{p}\alpha_{k}\right)^{2}
\\
&\le
\frac{\gamma^3 A_{K}^{2}}{(1-\gamma B_\mu)^2}
\mathsf{E}\sum_{j=1}^{q}
\left(\sum_{p=j+1}^{q+1}  \prod_{k=2}^{p} \alpha_{k}
\right)^{2}
\\
%&=
%\frac{A_{K}^{2}}{(1-\gamma_1 b_\mu)^2}
%\sum_{j=l}^{q} \gamma_{j}
%\prod_{k=l+1}^{j} \alpha_{k}^2
%\left(
%\sum_{p=j+1}^{q+1} \gamma_{p} \prod_{k=j+1}^{p} \alpha_{k}
%\right)^{2}
%\\
&\le
\frac{\gamma^3 A_{K}^{2}}{(1-\gamma B_\mu)^3}
\mathsf{E}\sum_{j=1}^{q}
\prod_{k=1}^{j} \alpha_{k}
\left(\sum_{p=j+1}^{q+1}  \prod_{k=j+1}^{p} \alpha_{k}
\right)^{2}
\end{align*}

Now, from the H\"older inequality, we obtain (with $\|X\|_p = (\mathsf{E}X^p)^{\frac{1}{p}}$)
\begin{align*}
\mathsf{E}\left[\sum_{j=1}^{q}\prod_{k=l}^{j}\alpha_{k}\left( \sum_{p=j+1}^{q+1}\prod_{k=j+1}^{p}\alpha_{k}\right)^{2}\right] & \leq\sum_{j=1}^{q}\left\Vert \prod_{k=1}^{j}\alpha_{k}\right\Vert _{2}\left\Vert \sum_{p=j+1}^{q+1} \prod_{k=j+1}^{p}\alpha_{k}\right\Vert _{4}^{2}\\
 & \leq\sum_{j=1}^{q} \left\Vert \prod_{k=1}^{j}\alpha_{k}\right\Vert _{2}\left(\sum_{p=j+1}^{q+1}\left\Vert \prod_{k=j+1}^{p}\alpha_{k}\right\Vert _{4}\right)^{2}.
\end{align*}
Now using the fact that $\prod\limits_{k=j+1}^{p}\alpha_{k} \leq \exp\left(-\sum\limits_{k=j+1}^{p}\gamma\mu'(X_{k-1})\right),$
we get
\[
\mathsf{E}\left[\sum_{j=1}^{q}\prod_{k=1}^{j}\alpha_{k}\left(\sum_{p=j+1}^{q+1}\prod_{k=j+1}^{p}\alpha_{k}\right)^{2}\right]\leq\sum_{j=1}^{q} \zeta^{1/2}_{1,j}(2) \left(\sum_{p=j+1}^{q+1} \zeta^{1/4}_{j+1,p}(4)\right)^{2},
\]
where we denote
\[
\zeta_{l,j}(u)=\mathsf{E}\left[e^{-u\sum_{k=l}^{j}\gamma\mu'(X_{k-1})}\right],\quad u>0.
\]
From Theorem~\ref{thm:ula_const_size} in Appendix~\ref{sec:appendix}, it follows that
\[
\mathsf{E}\left[e^{-u\gamma\left(\sum_{k=l}^{j}[\mu'(X_{k-1})-\pi_\gamma(\mu')]\right)}\right]\leq e^{u^2\varkappa (j-l-1)\gamma^{2}}
\]
with constant $\varkappa = ...$, where we used the fact that $\mu^{\prime}(x)$ is bounded. Then we have
\[
\mathsf{E}\left[e^{-s\sum_{k=l}^{j}\gamma_{k}\mu'(X_{k-1})}\right]\leq e^{s^2\varkappa \gamma^2(j-l-1)}e^{-s\gamma (j-l-1)\pi_\gamma(\mu')}.
\]
\denis{Please finish}
Furthermore, since $\mu\pi',\mu'\pi\in L^{1}(\mathbb{R})$, we have
\[
\pi(\mu')=\int\mu'(x)\pi(x)\,dx=-\int\mu(x)\pi'(x)\,dx=2\int\mu^{2}(x)\pi(x)\,dx>0
\]
yielding the final bound
\[
\zeta_{l,j}(s) \leq e^{s^2\kappa (j-l-1) \gamma^2}e^{-2s\gamma(j-l-1)\alpha}
\]
where $ \alpha = \int \mu^2(x)\pi(x)\,dx$.
Finally
\begin{align*}
\mathsf{E}\left[\sum_{j=1}^{q}\prod_{k=1}^{j}\alpha_{k}\left(\sum_{p=j+1}^{q+1}\prod_{k=j+1}^{p}\alpha_{k}\right)^{2}\right]  \lesssim
\end{align*}
Plugging into bound for conditional variance,
\begin{align*}
\mathsf{Var}\left[\sum_{p=l}^{q}\gamma_{p+1}\partial_{y_{l}}^{K}f\left(X_{p}\right)\Big|X_{l-1}\right] \leq
\frac{A_{K}^{2}e^{2\kappa \Gamma^*}}{4\alpha^2(1-\gamma_1 B_\mu)^3}
\end{align*}
\par
The proof is completed.
\end{proof}


\appendix
\section{Bounds for moments of  ULA}\label{sec:appendix}

We shall show that conditions {\bf (H1)} and {\bf (H2)} are sufficient to show that ULA kernel $R_\gamma$ satisfies the so-called drift condition:
\begin{definition}
\label{def:drift}
We say that the Markov kernel $R_\gamma$ satisfies Foster-Lyapunov drift condition if there exist a measurable function $V: X \rightarrow [1; +\infty)$, real numbers $\lambda \in (0,1)$ and $C > 0$ such that for any $x \in X$,
\begin{eqnarray*}
RV(x) \leq \lambda^{\gamma} V(x) + \gamma C
\end{eqnarray*}
\end{definition}


\begin{lem}
\label{lem:drift}
 Assume that the potential $U(x), x \in \mathbb{R}^d$ satisfies conditions {\bf (H1)} and {\bf (H2)} and without loss of generality consider $\nabla U(0) = 0$. Then the kernel $R_\gamma$ from \ref{eq:kernel} satisfies drift condition \ref{def:drift} for any $0 < \gamma < \overline{\gamma} = \frac{m}{4L^2}$ with drift function $V(x) = 1 + \|x\|^2$, constants $\lambda = \exp{\left(-\frac{m}{2}\right)}$, $C = \frac{25K_1^2}{8} + 2d + m$ with $K_1$ from \ref{eq:H3}, $m$ from {\bf (H2)}.
\end{lem}

\begin{proof} Consider $V(x) = 1 + \|x\|^2$, then
$$
R_{\gamma}V(x) = \int\limits_{\mathbb{R}^d}V(y)P_{\gamma}(x,dy) = \int\limits_{\mathbb{R}^d}(1+\|y\|^2)\frac{1}{(4\pi\gamma)^{\frac{d}{2}}}\exp{\left(-\frac{\|y-x+\gamma\nabla U(x)\|^2}{4\gamma}\right)}\,dy =
$$

$$
=1 + \frac{1}{(4\pi\gamma)^{\frac{d}{2}}}\int\limits_{\mathbb{R}^d}\|z + x - \gamma \nabla U(x)\|^2 \exp{\left(-\frac{\|z\|^2}{4\gamma}\right)}\,dz
$$
Let us first consider the case $x \notin B(0,K_1)$. Note that
$$
\|z+x-\gamma \nabla U(x)\|^2 = \|z\|^2 + 2 \langle z, x - \gamma \nabla U(x) \rangle + \gamma^2 \|x - \gamma \nabla U(x)\|^2
$$
and the linear term vanishes after integration, moreover, for $Z \sim \mathcal{N}(0,2\gamma I_d)$, it holds $\mathbb{E}\|Z\|^2 = 2\gamma d$. It remains to notice that due to {\bf H1} and {\bf H2} (which implies \ref{eq:H3}),
$$
\|x - \gamma \nabla U(x) \|^2 = \|x\|^2 - 2\gamma \langle \nabla U(x), x\rangle + \gamma^2 \|\nabla U(x)\|^2 \leq
$$

$$
 \leq (1-\gamma m)\|x\|^2 + 2\gamma^2 L^2 \|x\|^2
$$
Thus, plugging everything into expression for $R_{\gamma}V$ and using $\gamma < \frac{m}{4L}$, we obtain
$$
R_{\gamma}V(x) \leq (1-\gamma m + 2\gamma^2 L^2)V(x) + 2\gamma d + (\gamma m - 2\gamma^2 L^2) \leq \exp^{-\frac{\gamma m}{2}}V(x) + \gamma (2d + m)
$$
Now let $x \in B(0,K_1)$. Then simply using $\|x-\gamma \nabla U(x)\|^2 \leq 2(1+L\gamma)^2\|x\|^2$, we obtain
$$
R_{\gamma}V(x) \leq (1-\gamma m + 2\gamma^2 L^2)V(x) + \gamma\left((m - 2\gamma L^2)(1+\|x\|^2) + 2d + 2(1+L\gamma)^2\|x\|^2\right) \leq
$$

$$
\leq \exp^{-\frac{\gamma m}{2}}V(x) + \gamma(\frac{25K_1^2}{8} + 2d + m)
$$
\end{proof}

For now let us assume for simplicity that we run ULA with fixed step size $\gamma$. Then it is known that under assumption {\bf (H1)} corresponding Markov chain would have unique stationary distribution $\pi_\gamma$, which is different from $\pi$. Yet this chain will be $V-$geometrically ergodic due to \cite[Theorem~19.4.1]{douc:moulines:priouret:soulier:2018}. Namely, the following lemma holds:

\begin{lem}
\label{lem:v_ergodicity}
 Assume that the potential $U(x), x \in \mathbb{R}^d$ satisfies conditions {\bf (H1)} and {\bf (H2)}. Then for $0 < \gamma < \overline{\gamma}=\frac{m}{4L^2}$, for any $x \in X$ it holds
$$
d_V(\delta_xQ_{\boldsymbol{\gamma}}^{1,n},\pi_\gamma) \leq C\rho^n\left(V(x) + \pi_\gamma(V)\right)
$$
with $V(x) = 1 + \|x\|^2$ and constants
$$
C = \left(1 + \exp{\left(-\frac{m\gamma}{2}\right)}\right)\left(1+\frac{\overline{b}}{(1-\varepsilon)(1-\exp{\left(-\frac{m\gamma}{2}\right)} - \frac{2b}{1+d})}\right);
$$

$$
b = \gamma(\frac{25K_1^2}{8} + 2d + m); \quad \overline{b} = b\exp^{-\frac{m\gamma}{2}} + d; \quad \varepsilon = 2\Phi\left(-\frac{\sqrt{d}(1+L\gamma)}{2\sqrt{\gamma}}\right); \quad
$$

$$
\rho = \exp{\left(-\gamma\left(\frac{m}{2} - 2\frac{\frac{25K_1^2}{8} + 2d + m}{d}\right)\frac{\log{(1-\varepsilon)}}{\log{(1-\varepsilon)} + \log{(\exp{\left(-\frac{m\gamma}{2}\right)} + \frac{2b}{d+1})}}\right)}
$$
\end{lem}

\begin{proof} Note that the condition {\bf (H1)} implies that the Markov kernel $Q^{1,n}_{\boldsymbol{\gamma}}$ satisfies $(1,\varepsilon)$-Doeblin condition with $\varepsilon = 2\Phi\left(-\frac{\sqrt{d}(1+L\gamma)}{2\sqrt{\gamma}}\right)$. Together with drift condition \ref{eq:drift} it allows to apply \cite[Theorem~19.4.1]{douc:moulines:priouret:soulier:2018} with appropriate constants.
\end{proof}

Aforementioned lemmas allow us to bound the exponential moment of the additive functional of ULA. Namely, the following theorem holds

\begin{thm}
\label{thm:ula_const_size}
Suppose that ULA kernel \ref{eq:kernel} satisfies assumptions {\bf (H1)} and {\bf (H2)}, and let $X_1,\ldots,X_n,\ldots$ be generated by ULA with constant step size $\gamma$. Let $X_0 = x$ be fixed. Then for any bounded function $g(x) : \mathbb{R}^d \rightarrow \mathbb{R}, |g(x)| \leq M$, it holds
\begin{eqnarray*}
\mathsf{E}\left[\exp\left(-\rho\gamma\sum_{k=l}^{p}\left(g(X_{k})-\pi_{\gamma}(g)\right)\right)\right] \leq C_1\exp{\left(s^2 \kappa \gamma^2 (p-l+1)\right)}
\end{eqnarray*}
for some absolute constant $C_1$ not depending on $l,p$.
\end{thm}

\begin{proof} Conditions {\bf (H1)} and {\bf (H2)} imply that the Markov kernel $R_\gamma$ is $V-$ergodic (lemma \ref{lem:v_ergodicity}) and satisfies drift condition \ref{lem:drift}. Due to \cite[Theorem~3, Fact 3]{moulines:bound_dif:2019}, we obtain the following bound
\begin{align*}
\mathsf{E}_x\left[\exp\left(-s\gamma\sum_{k=l}^{p}\left(g(X_{k}) - \mathsf{E}_x g(X_k)\right)\right)\right] \leq \exp{\left(s^2 \kappa \gamma^2 (p-l+1)\right)}
\end{align*}
\sergey{I will put precise constant later}
with constant $\kappa = ...$. Note that
\begin{align*}
\mathsf{E}\left[\exp\left(-s\gamma\sum_{k=l}^{p}\left(g(X_{k})-\pi_{\gamma}(g)\right)\right)\right] =
\end{align*}

\begin{align*}
=\mathsf{E}_x\left[\exp{\left(-s\gamma\sum_{k=l}^{p}\left(g(X_{k}) - \mathsf{E}_x g(X_k)\right)\right)}\right]
\exp{\left(s\gamma\sum\limits_{k=l}^{p}\left(\pi_\gamma (g) - \mathsf{E}_x g(X_k)\right)\right)}
\end{align*}
Using lemma~\ref{lem:v_ergodicity},
\begin{align*}
|\mathsf{E} g(X_k) - \pi_\gamma(g)| \leq Md_V(\pi_\gamma, \delta_xQ^{1,n}_{\boldsymbol{\gamma}}) \leq CM\rho^k\left(V(x) + \pi_\gamma(V)\right)
\end{align*}
Hence,
\begin{align*}
\exp{\left(s\gamma\sum\limits_{k=l}^{p}\left(\pi_\gamma (g) - \mathsf{E}_x g(X_k)\right)\right)} \leq \exp{\left(s\gamma CM(V(x) + \pi_\gamma(V))\frac{\rho^l - \rho^{p+1}}{1-\rho}\right)}
\end{align*}
and the statement follows.
\end{proof}

Let us formulate and prove prove analogue of the previous theorem for inhomogeneous Markov chain with step size $\gamma_1,\ldots,\gamma_n,\ldots$. Note that under assumptions $\sum_{p=1}^{\infty} \gamma_p=\infty, \quad \sum_{p=1}^{\infty} \gamma^2_p<\infty$ and {\bf (H1)} the stationary distribution of ULA-based chain will be equal to $\pi$. We will need additional assumption
\begin{description}
\item[{\bf (H3)}] There exist such constants $\rho > 0, K_3 > 0$ that
\begin{eqnarray*}
d_V(\delta_xQ_{\boldsymbol{\gamma}}^{1,n},\pi) \leq K_3 V(x) \rho^n
\end{eqnarray*}
\end{description}

\begin{thm}
\label{th:exp_mom_ula}
Suppose that
\begin{eqnarray*}
\sum_{p=1}^{\infty} \gamma_p=\infty, \quad \sum_{p=1}^{\infty} \gamma^2_p<\infty.
\end{eqnarray*}
Under assumptions {\bf (H1)}, {\bf (H2)} and {\bf (H3)}, for any bounded function $g$ on $\mathbb{R}^d$
we have
%\begin{eqnarray*}
%\mathsf{E}\left[\left(\sum_{p=l}^{N+n}\omega_{p,n}^{N}g(X_{p})-\bar Q_l(X_l)\right)^{4}\right] \lesssim \frac{1}{\Gamma^4_{N+2,N+n+1}}
%\end{eqnarray*}
 %and
%\begin{eqnarray*}
%\bar Q_l(X_l)=\mathsf{E}\left[\left.\sum_{p=l}^{N+n}\omega_{p,n}^{N}f(X_{p})\right|X_{l}\right].
%\end{eqnarray*}
%Moreover,
\begin{eqnarray}
\label{eq:exp_decrease_step}
\mathsf{E}\left[\exp\left(-s\left(\sum_{k=l}^{p}\gamma_{k+1}g(X_{k})-\left(\sum_{k=l}^{p}\gamma_{k+1}\right)\pi(g)\right)\right)\right]\leq C
\end{eqnarray}
for all natural $0<l\leq p<\infty$ with constant $C$ not depending on $l,p.$. Moreover,
\begin{eqnarray}
\label{eq:var_decrease_step}
\mathsf{Var}_x{\left(\sum_{k=l}^{l+n-1}\frac{\gamma_{k+1}}{\Gamma_{l+1,l+n}}g(X_k) - \mathsf{E}_x \sum_{k=l}^{l+n-1}\frac{\gamma_{k+1}}{\Gamma_{l+1,l+n}}g(X_k)\right)} \leq \frac{C_1}{\Gamma_{l+1,l+n}}
\end{eqnarray}
\end{thm}
\begin{proof}

 Note that under assumptions {\bf (H1)}, {\bf (H2)} and {\bf (H3)} it holds due to \cite[Theorem~4]{moulines:bound_dif:2019} that
 \begin{align}
 \label{eq:exp_bounded_dif}
\mathsf{E}_x\left[\exp\left(-s\left(\sum_{k=l}^{p}\gamma_{k+1}g(X_{k}) - \mathsf{E}_x \left(\sum\limits_{k=l}^{p}\gamma_{k+1}g(X_k)\right)\right)\right)\right] \leq \exp{\left(s^2 \kappa \sum\limits_{k=l}^{p}\gamma_{k+1}^2\right)} \leq \exp{\left(s^2 \kappa \Gamma^*\right)}
\end{align}
\sergey{This theorem is not yet written in the second paper}
with $\Gamma^* = \sum\limits_{k=1}^{\infty}\gamma_k^2 < \infty$ and $\kappa = ...$. Using the same transformation as in theorem~\ref{thm:ula_const_size}, and using the bound
\begin{align*}
|\mathsf{E}_x f(X_k) - \pi(f)| \leq K_3 M \rho^{\Gamma_{1,k}}V(x)
\end{align*}
we obtain
\begin{align*}
\mathsf{E}\left[\exp\left(-s\left(\sum_{k=l}^{p}\gamma_{k+1}g(X_{k})-\left(\sum_{k=l}^{p}\gamma_{k+1}\right)\pi(g)\right)\right)\right] \leq \exp{\left(s^2 \kappa \Gamma^*\right)}\exp{\left(K_3 M V(x) \sum\limits_{k=l}^{p}\rho^{\Gamma_{1,k}}\gamma_{k+1}\right)} \leq
\end{align*}

\begin{align*}
\leq \exp{\left(s^2 \kappa \Gamma^*\right)}\exp{\left(K_3 M V(x) \rho^{\Gamma_{1,k}} \int\limits_{0}^{+\infty}\rho^y\,dy\right)} = \exp{\left(s^2 \kappa \Gamma^*\right)}\exp{\left(K_3 M V(x) \frac{\rho^{\Gamma_{1,k}}}{\ln{\rho}}\right)}
\end{align*}
\end{proof}
which proves \ref{eq:exp_decrease_step}. Note that from \ref{eq:exp_bounded_dif} by Taylor expansion for small $s$ we obtain
\begin{align*}
1 + \frac{s^2}{2}\mathsf{Var}_x{\left(\sum_{k=l}^{l+n-1}\gamma_{k+1}g(X_k) - \mathsf{E}_x \sum_{k=l}^{l+n-1}\gamma_{k+1}g(X_k)\right)} + O(s^3) \leq 1 + s^2 \kappa \sum\limits_{k=l+1}^{l+n}\gamma_{k}^2 + O(s^4)
\end{align*}
Since it holds for arbitarily small $s$, the variance is bounded by
\begin{align*}
\mathsf{Var}_x{\left(\sum_{k=l}^{l+n-1}\gamma_{k+1}g(X_k) - \mathsf{E}_x \sum_{k=l}^{l+n-1}\gamma_{k+1}g(X_k)\right)} \leq 2\kappa \sum\limits_{k=l+1}^{l+n}\gamma_{k}^2 \leq 2\kappa \Gamma_{l+1,l+n}
\end{align*}
Now we need to divide both parts by $\Gamma^2_{l+1,l+n}$ to get \ref{eq:var_decrease_step}. The proof is completed.
\bibliographystyle{plain}
\bibliography{refs-1}
\end{document}
