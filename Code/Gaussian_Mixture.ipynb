{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.polynomial as P\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "from joblib import Parallel, delayed\n",
    "import ZVnbrosse\n",
    "from potentials import GaussPotential,GaussMixture,GausMixtureIdent,GausMixtureSame\n",
    "from samplers import MCMC_sampler,Generate_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "N_b = 1*10**4 # Burn in period\n",
    "N_train = 1*10**4 # Number of samples on which we optimize\n",
    "N_test = 1*10**4 # Number of samples\n",
    "step = 0.2 # Step size\n",
    "n_traj_train = 1\n",
    "n_traj_test = 20 # Number of independent MCMC trajectories for test\n",
    "f_type = \"posterior_mean\"\n",
    "b_n_train = 20 #lag-window size\n",
    "b_n_test = int(np.round(N_test**(0.33)))\n",
    "print(b_n_test)\n",
    "degree = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose sampler type (currently only ULA is maintained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = {\"sampler\":\"ULA\",\"burn_type\":\"full\",\"main_type\":\"full\"} # Sampling method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncores =  4\n"
     ]
    }
   ],
   "source": [
    "d = 2\n",
    "mu = 0.5*np.array([1.0,1.0],dtype = float)\n",
    "#mu_1 = np.array([-1.0])\n",
    "#mu_2 = np.array([1.0])\n",
    "#Sigma_1 = np.array([[1.0]])\n",
    "#Sigma_2 = np.array([[1.0]])\n",
    "#Sigma = GenerateSigma(d,rand_seed = 777,eps = 0.1) #covariation matrix \n",
    "p = 0.5\n",
    "#Cur_pot = GausMixtureSame(Sigma,mu,p)\n",
    "#Cur_pot = GaussMixture(Sigma_1,Sigma_2,mu_1,mu_2,p)\n",
    "Cur_pot = GausMixtureIdent(mu,p)\n",
    "#sample for variance reduction\n",
    "seed = 777\n",
    "if sampler[\"sampler\"] == \"ULA\":\n",
    "    res = Generate_train(n_traj_train, sampler, Cur_pot, step, N_b, N_train, d)\n",
    "    res = np.asarray(res)\n",
    "    traj,traj_grad = res[:,0,:,:],res[:,1,:,:]\n",
    "else:\n",
    "    res = Generate_train(n_traj_train, sampler, Cur_pot, step, N_b, N_train, d)\n",
    "    traj = []\n",
    "    traj_grad = []\n",
    "    for i in range(len(res)):\n",
    "        traj.append(res[i][0])\n",
    "        traj_grad.append(res[i][1])\n",
    "        print(\"accepted = \",res[i][2])\n",
    "    traj = np.asarray(traj)\n",
    "    traj_grad = np.asarray(traj_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_arr = np.array([0]) # Taking the second index (not intercept)\n",
    "params = None    \n",
    "f_vals = set_function(f_type,traj,inds_arr,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f_grad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-399f4f8aebdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#burn-in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_burn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx_cur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_cur\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mf_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cur\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m#training sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f_grad' is not defined"
     ]
    }
   ],
   "source": [
    "#generate samples from mixture of normals\n",
    "N_burn = 10000\n",
    "N_train = 10000\n",
    "gamma = 0.2\n",
    "N_traj_train = 10\n",
    "X_train = np.zeros((N_traj_train,N_train),dtype = float)\n",
    "\n",
    "for j in range(N_traj_train):\n",
    "    np.random.seed(142+j)\n",
    "    x0 = np.random.randn()\n",
    "    x_cur = x0\n",
    "    #burn-in\n",
    "    for i in range(N_burn):\n",
    "        x_cur = x_cur - gamma*f_grad(x_cur) + np.sqrt(2*gamma)*np.random.randn()\n",
    "    #training sample\n",
    "    for i in range(N_train):\n",
    "        X_train[j,i] = x_cur\n",
    "        x_cur = x_cur - gamma*f_grad(x_cur) + np.sqrt(2*gamma)*np.random.randn()\n",
    "X_last = X_train[0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(X_train[0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize coefficients by solving regression with polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree\n",
    "max_deg = 5\n",
    "#lag order\n",
    "lag = 10\n",
    "#polynomial coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coefs_poly[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For particular example of gaussian distribution, we might compute coefficients of $Q_{p-l}$ analytically in closed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_poly_theor = np.zeros_like(coefs_poly)\n",
    "for i in range(lag):\n",
    "    if i == 0:\n",
    "        coefs_poly_theor[i,0] = 0.0\n",
    "        coefs_poly_theor[i,1] = 1.0\n",
    "    else:\n",
    "        coefs_poly_theor[i,0] = a - a*(1-gamma/sigma**2)**(i)\n",
    "        coefs_poly_theor[i,1] = (1-gamma/sigma**2)**(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coefs_poly = coefs_poly_theor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_lag = 1\n",
    "N_pts = 500\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Testing regression model\",fontsize=20)\n",
    "plt.plot(X_train[0,cur_lag:N_pts],color='r',label='true function')\n",
    "plt.plot(P.polynomial.polyval(X_train[0,:N_pts-cur_lag],coefs_poly[cur_lag,:]),color='g',label = 'practical approximation')\n",
    "#plt.plot(P.polynomial.polyval(X_train[0,:N_pts-cur_lag],coefs_poly_theor[cur_lag,:]),color='b',label = 'theoretical approximation')\n",
    "plt.legend(loc = 'lower right',fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.special.factorial2(21, exact=False)\n",
    "moments_stand_norm = np.zeros(2*max_deg+1,dtype = float)\n",
    "moments_stand_norm[0] = 1.0\n",
    "moments_stand_norm[1] = 0.0\n",
    "for i in range(len(moments_stand_norm)-2):\n",
    "    moments_stand_norm[i+2] = sp.special.factorial2(i+1, exact=False)\n",
    "#eliminate odd\n",
    "moments_stand_norm[1::2] = 0\n",
    "print(moments_stand_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_burn = 10000\n",
    "N_test = 2000 #size of test part\n",
    "N_min = 1 #minimal number of observations to compute \\pi_N\n",
    "gamma = 0.2\n",
    "X_test = np.zeros(N_test,dtype = float)\n",
    "Noise = np.zeros_like(X_test)\n",
    "N_traj = 10\n",
    "\n",
    "test_stat_vanilla = np.zeros((N_traj,N_test),dtype = float)\n",
    "test_stat_vr = np.zeros((N_traj,N_test),dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(N_traj):\n",
    "    np.random.seed(1453 + ind)\n",
    "    x0 = np.random.randn()\n",
    "    x_cur = x0\n",
    "    #burn-in\n",
    "    for i in range(N_burn):\n",
    "        x_cur = x_cur - gamma*f_grad(x_cur) + np.sqrt(2*gamma)*np.random.randn()\n",
    "    #training sample\n",
    "    #x_cur = X_last\n",
    "    for i in range(N_test):\n",
    "        Noise[i] = np.random.randn()\n",
    "        x_cur = x_cur - gamma*f_grad(x_cur) + np.sqrt(2*gamma)*Noise[i]\n",
    "        X_test[i] = x_cur\n",
    "    #compute polynomials at Z_l\n",
    "    poly_vals = np.zeros((max_deg+1,N_test),dtype = float)\n",
    "    for k in range(max_deg+1):\n",
    "        c = np.zeros(max_deg+1)\n",
    "        c[k] = 1\n",
    "        poly_vals[k,:] = P.hermite_e.hermeval(Noise,c)/np.sqrt(sp.special.factorial(k))\n",
    "    f_vals_vanilla = set_func(X_test)\n",
    "    cvfs = np.zeros_like(f_vals_vanilla)\n",
    "    for i in range(1,len(cvfs)):\n",
    "        #start computing a_{p-l} coefficients\n",
    "        num_poly = min(lag,i)\n",
    "        a_vals = np.zeros((num_poly,max_deg+1),dtype = float)\n",
    "        for npol in range(num_poly):#for a fixed lag Q function\n",
    "            #compute \\hat{a} with fixed lag\n",
    "            x = X_test[i-1-npol]#should be -2 here?\n",
    "            a_cur = np.zeros(max_deg+1,dtype=float)\n",
    "            for m in range(len(coefs_poly[0])):\n",
    "                poly_vspom = np.zeros(max_deg+1,dtype=float)\n",
    "                for u in range(m+1):\n",
    "                    poly_vspom[u] = ((x-gamma*f_grad(x))**(m-u))*((np.sqrt(2*gamma))**u)*sp.special.binom(m,u)\n",
    "                #print(poly_vspom)\n",
    "                a_cur = P.polynomial.polyadd(a_cur,coefs_poly[npol,m]*poly_vspom)\n",
    "            #print(\"a_cur\")\n",
    "            #print(a_cur)\n",
    "            for k in range(1,max_deg+1):\n",
    "                c = np.zeros(max_deg+1)\n",
    "                c[k] = 1\n",
    "                herm_coef = P.hermite_e.herme2poly(c)\n",
    "                #normalize now\n",
    "                herm_coef = herm_coef / np.sqrt(sp.special.factorial(k))\n",
    "                integr_coefs = P.polynomial.polymul(herm_coef,a_cur)\n",
    "                #Note that a_vals are stored in reversed order\n",
    "                a_vals[-(npol+1),k] = np.dot(integr_coefs,moments_stand_norm[:len(integr_coefs)])\n",
    "            #OK, now I have coefficients of the polynomial, and I need to integrate it w.r.t. Gaussian measure\n",
    "        #print(a_vals.shape)\n",
    "        #print(a_vals)\n",
    "        cvfs[i] = np.sum(a_vals*(poly_vals[:,i-num_poly+1:i+1].T))\n",
    "        #cvfs[i] = np.sum(np.mean(a_vals,axis = 0))\n",
    "        if (i%100 == 0):\n",
    "            print(\"100 observations proceeded\")\n",
    "        #save results\n",
    "        test_stat_vanilla[ind,i] = np.mean(f_vals_vanilla[1:(i+1)])\n",
    "        test_stat_vr[ind,i] = test_stat_vanilla[ind,i] - np.sum(cvfs[:i])/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_stat_vanilla[1,N_min:N_min+100])\n",
    "print(test_stat_vr[1,N_min:N_min+100])\n",
    "print(test_stat_vanilla[1,-1])\n",
    "print(test_stat_vr[1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_vanilla = np.var(test_stat_vanilla,axis = 0)\n",
    "vars_adj = np.var(test_stat_vr,axis = 0)\n",
    "print(vars_vanilla[N_min:N_min+10])\n",
    "print(vars_adj[N_min:N_min+10])\n",
    "print(np.mean(vars_adj[N_min:]/vars_vanilla[N_min:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(vars_adj[-10:]/vars_vanilla[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
